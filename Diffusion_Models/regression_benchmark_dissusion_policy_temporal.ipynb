{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2554,"status":"ok","timestamp":1738074560054,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"},"user_tz":300},"id":"YkI_GtjaTfqd","outputId":"9978eb17-2c34-4f8b-a3fb-65a07b124087"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n"]}],"source":["\n","#mount drive\n","from google.colab import drive\n","drive.mount('/content/MyDrive')\n","import seaborn as sns\n","sns.set_theme(\"paper\")\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ETk_LSRPvM--","executionInfo":{"status":"ok","timestamp":1738074560055,"user_tz":300,"elapsed":17,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"outputs":[],"source":["# @title Initialize Config\n","\n","import torch\n","import numpy\n","class Config:\n","    def __init__(self, **kwargs):\n","        self.channels_imu_acc = kwargs.get('channels_imu_acc', [])\n","        self.channels_imu_gyr = kwargs.get('channels_imu_gyr', [])\n","        self.channels_joints = kwargs.get('channels_joints', [])\n","        self.channels_emg = kwargs.get('channels_emg', [])\n","        self.seed = kwargs.get('seed', 42)\n","        self.data_folder_name = kwargs.get('data_folder_name', 'default_data_folder_name')\n","        self.dataset_root = kwargs.get('dataset_root', 'default_dataset_root')\n","        self.imu_transforms = kwargs.get('imu_transforms', [])\n","        self.joint_transforms = kwargs.get('joint_transforms', [])\n","        self.emg_transforms = kwargs.get('emg_transforms', [])\n","        self.input_format = kwargs.get('input_format', 'csv')\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","config = Config(\n","    data_folder_name='/content/MyDrive/MyDrive/sd_datacollection_v4/all_subjects_data_final.h5',\n","    dataset_root='/content/datasets',\n","    input_format=\"csv\",\n","    channels_imu_acc=['ACCX1', 'ACCY1', 'ACCZ1','ACCX2', 'ACCY2', 'ACCZ2', 'ACCX3', 'ACCY3', 'ACCZ3', 'ACCX4', 'ACCY4', 'ACCZ4', 'ACCX5', 'ACCY5', 'ACCZ5', 'ACCX6', 'ACCY6', 'ACCZ6'],\n","    channels_imu_gyr=['GYROX1', 'GYROY1', 'GYROZ1', 'GYROX2', 'GYROY2', 'GYROZ2', 'GYROX3', 'GYROY3', 'GYROZ3', 'GYROX4', 'GYROY4', 'GYROZ4', 'GYROX5', 'GYROY5', 'GYROZ5', 'GYROX6', 'GYROY6', 'GYROZ6'],\n","    channels_joints=['elbow_flex_r', 'arm_flex_r', 'arm_add_r'],\n","    channels_emg=['IM EMG4', 'IM EMG5', 'IM EMG6'],\n",")\n","\n","#set seeds\n","torch.manual_seed(config.seed)\n","numpy.random.seed(config.seed)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yvI4iVwAvg7y","executionInfo":{"status":"ok","timestamp":1738074560056,"user_tz":300,"elapsed":17,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"outputs":[],"source":["class DataSharder:\n","    def __init__(self, config, split):\n","        self.config = config\n","        self.h5_file_path = config.data_folder_name  # Path to the HDF5 file\n","        self.split = split\n","\n","    def load_data(self, subjects, window_length, window_overlap, dataset_name):\n","        print(f\"Processing subjects: {subjects} with window length: {window_length}, overlap: {window_overlap}\")\n","\n","        self.window_length = window_length\n","        self.window_overlap = window_overlap\n","\n","        # Process the data from the HDF5 file\n","        self._process_and_save_patients_h5(subjects, dataset_name)\n","\n","    def _process_and_save_patients_h5(self, subjects, dataset_name):\n","        # Open the HDF5 file\n","        with h5py.File(self.h5_file_path, 'r') as h5_file:\n","            dataset_folder = os.path.join(self.config.dataset_root, dataset_name, self.split).replace(\"subject\", \"\").replace(\"__\", \"_\")\n","            print(\"Dataset folder:\", dataset_folder)\n","\n","            if os.path.exists(dataset_folder):\n","                print(\"Dataset Exists, Skipping...\")\n","                return\n","\n","            os.makedirs(dataset_folder, exist_ok=True)\n","            print(\"Dataset folder created: \", dataset_folder)\n","\n","            for subject_id in tqdm(subjects, desc=\"Processing subjects\"):\n","                subject_key = subject_id\n","                if subject_key not in h5_file:\n","                    print(f\"Subject {subject_key} not found in the HDF5 file. Skipping.\")\n","                    continue\n","\n","                subject_data = h5_file[subject_key]\n","                session_keys = list(subject_data.keys())  # Sessions for this subject\n","\n","                for session_id in session_keys:\n","                    session_data_group = subject_data[session_id]\n","\n","                    for sessions_speed in session_data_group.keys():\n","                        session_data = session_data_group[sessions_speed]\n","\n","                        # Extract IMU, EMG, and Joint data as numpy arrays\n","                        imu_data, imu_columns = self._extract_channel_data(session_data, self.config.channels_imu_acc + self.config.channels_imu_gyr)\n","                        emg_data, emg_columns = self._extract_channel_data(session_data, self.config.channels_emg)\n","                        joint_data, joint_columns = self._extract_channel_data(session_data, self.config.channels_joints)\n","\n","                        # Shard the data into windows and save each window\n","                        self._save_windowed_data(imu_data, emg_data, joint_data, subject_key, session_id,sessions_speed, dataset_folder, imu_columns, emg_columns, joint_columns)\n","\n","    def _save_windowed_data(self, imu_data, emg_data, joint_data, subject_key, session_id, session_speed, dataset_folder, imu_columns, emg_columns, joint_columns):\n","        window_size = self.window_length\n","        overlap = self.window_overlap\n","        step_size = window_size - overlap\n","\n","        # Path to the CSV log file\n","        csv_file_path = os.path.join(dataset_folder, '..', f\"{self.split}_info.csv\")\n","\n","        # Ensure the folder exists\n","        os.makedirs(dataset_folder, exist_ok=True)\n","\n","        # Prepare CSV log headers (ensure the columns are 'file_name' and 'file_path')\n","        csv_headers = ['file_name', 'file_path']\n","\n","        # Create or append to the CSV log file\n","        file_exists = os.path.isfile(csv_file_path)\n","        with open(csv_file_path, mode='a', newline='') as csv_file:\n","            writer = csv.writer(csv_file)\n","\n","            # Write the headers only if the file is new\n","            if not file_exists:\n","                writer.writerow(csv_headers)\n","\n","            # Determine the total data length based on the minimum length across the data sources\n","            total_data_length = min(imu_data.shape[1], emg_data.shape[1], joint_data.shape[1])\n","\n","            # Adjust the starting point for windows based on total data length\n","            start = 2000 if total_data_length > 4000 else 0\n","\n","            # Ensure that each window across imu_data, emg_data, and joint_data has the same shape before concatenation\n","            for i in range(start, total_data_length - window_size + 1, step_size):\n","                imu_window = imu_data[:, i:i + window_size]\n","                emg_window = emg_data[:, i:i + window_size]\n","                joint_window = joint_data[:, i:i + window_size]\n","\n","                # Check if the window sizes are valid\n","                if imu_window.shape[1] == window_size and emg_window.shape[1] == window_size and joint_window.shape[1] == window_size:\n","                    # Convert windowed data to pandas DataFrame\n","\n","\n","\n","                    imu_df = pd.DataFrame(imu_window.T, columns=imu_columns)\n","                    emg_df = pd.DataFrame(emg_window.T, columns=emg_columns)\n","                    joint_df = pd.DataFrame(joint_window.T, columns=joint_columns)\n","\n","\n","\n","                    # Concatenate the data along the column axis\n","                    combined_df = pd.concat([imu_df, emg_df, joint_df], axis=1)\n","\n","                    # Save the combined windowed data as a CSV file\n","                    file_name = f\"{subject_key}_{session_id}_{session_speed}_win_{i}_ws{window_size}_ol{overlap}.csv\"\n","                    file_path = os.path.join(dataset_folder, file_name)\n","                    combined_df.to_csv(file_path, index=False)\n","\n","                    # Log the file name and path in the CSV (in the correct columns)\n","                    writer.writerow([file_name, file_path])\n","                else:\n","                    print(f\"Skipping window {i} due to mismatched window sizes.\")\n","\n","    def _extract_channel_data(self, session_data, channels):\n","      extracted_data = []\n","      new_column_names = []  # Initialize here\n","\n","      if isinstance(session_data, h5py.Dataset):\n","          if session_data.dtype.names:\n","              # Compound dataset\n","              column_names = session_data.dtype.names\n","              for channel in channels:\n","                  if channel in column_names:\n","                      channel_data = session_data[channel][:]\n","                      channel_data = pd.to_numeric(channel_data, errors='coerce')\n","                      df = pd.DataFrame(channel_data)\n","                      df_interpolated = df.interpolate(method='linear', axis=0, limit_direction='both')\n","                      extracted_data.append(df_interpolated.to_numpy().flatten())\n","                      new_column_names.append(channel)  # Populate here\n","                  else:\n","                      print(f\"Channel {channel} not found in compound dataset.\")\n","          else:\n","              # Simple dataset\n","              column_names = list(session_data.attrs.get('column_names', []))\n","              assert len(column_names) > 0, \"column_names not found in dataset attributes\"\n","              for channel in channels:\n","                  if channel in column_names:\n","                      col_idx = column_names.index(channel)\n","                      channel_data = session_data[:, col_idx]\n","                      channel_data = pd.to_numeric(channel_data, errors='coerce')\n","                      df = pd.DataFrame(channel_data)\n","                      df_interpolated = df.interpolate(method='linear', axis=0, limit_direction='both')\n","                      extracted_data.append(df_interpolated.to_numpy().flatten())\n","                      new_column_names.append(channel)\n","                  else:\n","                      print(f\"Channel {channel} not found in session data.\")\n","\n","      return np.array(extracted_data), new_column_names\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"uCr-VlFC8Nu7","executionInfo":{"status":"ok","timestamp":1738074560056,"user_tz":300,"elapsed":16,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"outputs":[],"source":["# File: dataset.py\n","\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","class ImuJointPairDataset(Dataset):\n","    def __init__(\n","        self,\n","        config,\n","        subjects,\n","        window_length,\n","        window_overlap,\n","        split='train',\n","        feature_stats=None,\n","        dataset_train_name='train',\n","        dataset_test_name='test'\n","    ):\n","        self.config = config\n","        self.split = split\n","        self.subjects = subjects\n","        self.window_length = window_length\n","        self.window_overlap = window_overlap if split == 'train' else 0\n","        self.input_format = config.input_format\n","        self.channels_imu_acc = config.channels_imu_acc\n","        self.channels_imu_gyr = config.channels_imu_gyr\n","        self.channels_joints = config.channels_joints\n","        self.channels_emg = config.channels_emg\n","\n","        # Optional feature statistics for normalization (used for obs only)\n","        self.feature_stats = feature_stats\n","\n","        # You can adjust these ranges to match actual human-motion bounds\n","        self.joint_ranges = {\n","            'elbow_flex_r': (0, 150),   # Example: elbow flex from 0 to 150 deg\n","            'arm_flex_r':   (-90, 180),\n","            'arm_add_r':    (-120, 90)\n","        }\n","\n","        subjects_str = \"_\".join(map(str, subjects)).replace('subject', '').replace('__', '_')\n","        if split == 'train':\n","            dataset_name = f\"dataset_wl{self.window_length}_ol{self.window_overlap}_train{subjects_str}\"\n","        else:\n","            dataset_name = f\"dataset_wl{self.window_length}_ol{self.window_overlap}_test{subjects_str}\"\n","        self.dataset_name = dataset_name\n","        self.root_dir = os.path.join(self.config.dataset_root, self.dataset_name)\n","\n","        # Prepare sharded data if not found\n","        self.ensure_resharded(subjects, dataset_train_name if split == 'train' else dataset_test_name)\n","\n","        # Load info CSV\n","        info_path = os.path.join(self.root_dir, f\"{split}_info.csv\")\n","        self.data = pd.read_csv(info_path)\n","\n","        # If training set and no feature_stats passed, compute obs stats here\n","        if split == 'train' and feature_stats is None:\n","            self.feature_stats = self.compute_feature_stats()\n","\n","    def ensure_resharded(self, subjects, dataset_name):\n","        if not os.path.exists(self.root_dir):\n","            print(f\"Sharded data not found at {self.root_dir}. Resharding...\")\n","            data_sharder = DataSharder(self.config, self.split)\n","            data_sharder.load_data(\n","                subjects,\n","                window_length=self.window_length,\n","                window_overlap=self.window_overlap,\n","                dataset_name=self.dataset_name\n","            )\n","        else:\n","            print(f\"Sharded data found at {self.root_dir}. Skipping resharding.\")\n","\n","    def compute_feature_stats(self):\n","        all_obs = []\n","        # We won't do standard stats on joints since we'll manually min-max scale them\n","        for idx in range(len(self.data)):\n","            obs_data, _ = self.__getitem__(idx, normalize=False)\n","            # obs_data is shape (#windows, window_length, obs_channels)\n","            # We'll flatten across windows/time to get a single large array\n","            obs_data_flat = obs_data.reshape(-1, obs_data.shape[-1])\n","            all_obs.append(obs_data_flat)\n","        all_obs = np.vstack(all_obs)\n","        obs_stats = {'mean': np.mean(all_obs, axis=0), 'std': np.std(all_obs, axis=0)}\n","        return {'obs': obs_stats}\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx, normalize=True):\n","        file_name = self.data.iloc[idx, 0]\n","        file_path = os.path.join(self.root_dir, self.split, file_name)\n","        if self.input_format == \"csv\":\n","            combined_data = pd.read_csv(file_path)\n","        else:\n","            raise ValueError(\"Unsupported input format\")\n","\n","        obs_data, joint_data = self._extract_and_transform(combined_data, normalize)\n","        obs_windows = self.create_windows(obs_data, self.window_length, self.window_overlap)\n","        joint_windows = self.create_windows(joint_data, self.window_length, self.window_overlap)\n","        return obs_windows, joint_windows\n","\n","    def create_windows(self, data, window_length, window_overlap):\n","        stride = window_length - window_overlap\n","        return np.array([\n","            data[i:i + window_length] for i in range(0, len(data) - window_length + 1, stride)\n","        ])\n","\n","    def _extract_and_transform(self, combined_data, normalize):\n","        imu_data_acc = self._extract_channels(combined_data, self.channels_imu_acc)\n","        imu_data_gyr = self._extract_channels(combined_data, self.channels_imu_gyr)\n","        emg_data = self._extract_channels(combined_data, self.channels_emg)\n","        joint_data = self._extract_channels(combined_data, self.channels_joints)\n","\n","        obs_data = np.concatenate([imu_data_acc, imu_data_gyr, emg_data], axis=1)\n","\n","        if normalize and self.feature_stats:\n","            obs_stats = self.feature_stats['obs']\n","            obs_data = (obs_data - obs_stats['mean']) / obs_stats['std']\n","\n","            # Manually scale each joint channel to [-1, 1]\n","            for i, channel in enumerate(self.channels_joints):\n","                min_val, max_val = self.joint_ranges[channel]\n","                joint_data[:, i] = 2.0 * (joint_data[:, i] - min_val) / (max_val - min_val) - 1.0\n","\n","        return obs_data, joint_data\n","\n","    def _extract_channels(self, combined_data, channels):\n","        if self.input_format == \"csv\":\n","            return combined_data[channels].values\n","        raise ValueError(\"Unsupported format\")\n","\n","\n","def create_base_data_loaders(\n","    config,\n","    train_subjects,\n","    test_subjects,\n","    window_length=100,\n","    window_overlap=75,\n","    batch_size=64,\n","    dataset_train_name='train',\n","    dataset_test_name='test'\n","):\n","    # Create the training dataset and compute feature statistics\n","    train_dataset = ImuJointPairDataset(\n","        config=config,\n","        subjects=train_subjects,\n","        window_length=window_length,\n","        window_overlap=window_overlap,\n","        split='train',\n","        dataset_train_name=dataset_train_name\n","    )\n","    feature_stats = train_dataset.feature_stats  # Save computed stats\n","\n","    # Create the test dataset with normalization stats from training\n","    test_dataset = ImuJointPairDataset(\n","        config=config,\n","        subjects=test_subjects,\n","        window_length=window_length,\n","        window_overlap=0,  # Ensure no overlap for test\n","        split='test',\n","        feature_stats=feature_stats,  # Use training stats for normalization\n","        dataset_test_name=dataset_test_name\n","    )\n","\n","    # Split training dataset into train and validation sets\n","    train_size = int(0.9 * len(train_dataset))\n","    val_size = len(train_dataset) - train_size\n","    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","    # Assign training stats to the validation dataset\n","    val_dataset.dataset.feature_stats = feature_stats\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n"]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import math\n","\n","\n","class SinusoidalPosEmb(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        device = x.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = x[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","        return emb\n","\n","\n","class Downsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.Conv1d(dim, dim, 3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Upsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose1d(dim, dim, kernel_size=4, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Conv1dBlock(nn.Module):\n","    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n","        super().__init__()\n","        # Adjust n_groups to ensure divisibility\n","        n_groups = min(n_groups, out_channels)  # Ensure n_groups <= out_channels\n","        if out_channels % n_groups != 0:\n","            n_groups = 1  # Fallback to a single group (LayerNorm equivalent)\n","\n","        self.block = nn.Sequential(\n","            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n","            nn.GroupNorm(n_groups, out_channels),  # Updated n_groups\n","            nn.Mish(),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","\n","class ConditionalResidualBlock1D(nn.Module):\n","    def __init__(self, in_channels, out_channels, cond_dim, kernel_size=3, n_groups=8):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([\n","            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups),\n","            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups),\n","        ])\n","        cond_channels = out_channels * 2\n","        self.out_channels = out_channels\n","        self.cond_encoder = nn.Sequential(\n","            nn.Mish(),\n","            nn.Linear(cond_dim, cond_channels),\n","            nn.Unflatten(-1, (cond_channels, 1))\n","        )\n","        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n","\n","    def forward(self, x, cond):\n","        out = self.blocks[0](x)\n","        embed = self.cond_encoder(cond)\n","        embed = embed.view(embed.shape[0], 2, self.out_channels, 1)\n","        scale, bias = embed[:, 0], embed[:, 1]\n","        out = scale * out + bias\n","        out = self.blocks[1](out)\n","        return out + self.residual_conv(x)\n","\n","\n","class ConditionalUnet1D(nn.Module):\n","    def __init__(\n","        self, input_dim, global_cond_dim, diffusion_step_embed_dim=256,\n","        down_dims=[256, 512, 1024], kernel_size=5, n_groups=8\n","    ):\n","        super().__init__()\n","        self.diffusion_step_encoder = nn.Sequential(\n","            SinusoidalPosEmb(diffusion_step_embed_dim),\n","            nn.Linear(diffusion_step_embed_dim, diffusion_step_embed_dim * 4),\n","            nn.Mish(),\n","            nn.Linear(diffusion_step_embed_dim * 4, diffusion_step_embed_dim),\n","        )\n","        cond_dim = diffusion_step_embed_dim + global_cond_dim\n","        self.down_modules = nn.ModuleList([])\n","        self.up_modules = nn.ModuleList([])\n","\n","        # Downsampling Path\n","        prev_dim = input_dim\n","        for out_dim in down_dims:\n","            self.down_modules.append(nn.ModuleList([\n","                Conv1dBlock(prev_dim, out_dim, kernel_size, n_groups),\n","                nn.Conv1d(out_dim, out_dim, kernel_size=3, stride=2, padding=1)\n","            ]))\n","            prev_dim = out_dim\n","\n","        # Upsampling Path\n","        for in_dim, out_dim in zip(down_dims[::-1], down_dims[::-1][1:] + [input_dim]):\n","            self.up_modules.append(nn.ModuleList([\n","                Conv1dBlock(in_dim * 2, out_dim, kernel_size, n_groups),\n","                nn.ConvTranspose1d(out_dim, out_dim, kernel_size=4, stride=2, padding=1)\n","            ]))\n","\n","        self.final_conv = nn.Conv1d(down_dims[0], input_dim, kernel_size=1)\n","\n","    def forward(self, sample, timestep, global_cond):\n","        sample = sample.transpose(1, 2)\n","        diff_embed = self.diffusion_step_encoder(timestep)\n","        global_cond = torch.cat([global_cond, diff_embed], dim=-1)\n","\n","        skips = []\n","        for scale, down in self.down_modules:\n","            sample = scale(sample)\n","            skips.append(sample)\n","            sample = down(sample)\n","\n","        for scale, up in self.up_modules:\n","            skip = skips.pop()\n","            sample = torch.cat([sample, skip], dim=1)\n","            sample = scale(sample)\n","            sample = up(sample)\n","\n","        return self.final_conv(sample).transpose(1, 2)\n","\n","\n","\n"],"metadata":{"id":"dRTITfpAzvef","executionInfo":{"status":"ok","timestamp":1738075269919,"user_tz":300,"elapsed":185,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from diffusers import DDPMScheduler\n","from torch.optim import AdamW\n","from scipy.stats import pearsonr\n","import matplotlib.cm as cm\n","\n","# Joint ranges for unscaling from [-1,1]\n","joint_ranges = {\n","    'elbow_flex_r': (0, 150),   # Example: elbow flex from 0 to 150 deg\n","    'arm_flex_r':   (-90, 180),\n","    'arm_add_r':    (-120, 90)\n","}\n","\n","def unscale_joints(scaled_data, joint_names):\n","    unscaled = scaled_data.copy()\n","    for i, ch in enumerate(joint_names):\n","        min_val, max_val = joint_ranges[ch]\n","        unscaled[:, i] = 0.5 * (unscaled[:, i] + 1.0) * (max_val - min_val) + min_val\n","    return unscaled\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from diffusers import DDPMScheduler\n","from torch.optim import AdamW\n","from diffusers.training_utils import EMAModel\n","import matplotlib.cm as cm\n","\n","\n","def compute_loss(prediction, target, slope_weight=1.0, long_horizon_weight=0.1):\n","    # MSE for current time step\n","    mse_loss = F.mse_loss(prediction, target)\n","\n","    # Emphasize slopes\n","    grad_pred = torch.diff(prediction, dim=1)\n","    grad_target = torch.diff(target, dim=1)\n","    slope_loss = F.mse_loss(grad_pred, grad_target)\n","\n","    # Long-horizon consistency loss\n","    long_horizon_loss = F.mse_loss(prediction[:, :-1], target[:, 1:])\n","\n","    return mse_loss + slope_weight * slope_loss + long_horizon_weight * long_horizon_loss\n","\n","\n","def unscale_joints(scaled_data, joint_names, joint_ranges):\n","    unscaled = scaled_data.copy()\n","    for i, ch in enumerate(joint_names):\n","        min_val, max_val = joint_ranges[ch]\n","        unscaled[:, i] = 0.5 * (unscaled[:, i] + 1.0) * (max_val - min_val) + min_val\n","    return unscaled\n","\n","\n","def sample_trajectories(model, noise_scheduler, obs_sample, num_samples, device):\n","    B, T, joint_channels = obs_sample.shape\n","    global_cond = obs_sample.reshape(B, -1)\n","    noise_scheduler.timesteps = noise_scheduler.timesteps.to(device)\n","    predictions = []\n","    for _ in range(num_samples):\n","        sample = torch.randn(B, T, joint_channels, device=device)\n","        for step_id in noise_scheduler.timesteps:\n","            t = torch.tensor([step_id], device=sample.device)\n","            noise_pred = model(sample, t, global_cond)\n","            step_output = noise_scheduler.step(noise_pred, t, sample)\n","            sample = step_output.prev_sample\n","        predictions.append(sample.detach().cpu().numpy())\n","    return np.concatenate(predictions, axis=0)\n","\n","\n","def train_diffusion_policy(\n","    train_loader, val_loader, test_loader, device, num_epochs=50, lr=1e-4,\n","    num_diffusion_iters=100, early_stop_patience=5\n","):\n","    best_val_loss = float('inf')\n","    no_improve_epochs = 0\n","    example_batch = next(iter(train_loader))\n","    obs_ex, joints_ex = example_batch\n","    input_dim = joints_ex.shape[-1]\n","    global_cond_dim = obs_ex.shape[-1] * obs_ex.shape[-2]\n","\n","    model = ConditionalUnet1D(\n","        input_dim=input_dim,\n","        global_cond_dim=global_cond_dim,\n","        diffusion_step_embed_dim=256,\n","        down_dims=[256, 512, 1024],\n","        kernel_size=5,\n","        n_groups=8\n","    ).to(device)\n","\n","    noise_scheduler = DDPMScheduler(\n","        num_train_timesteps=num_diffusion_iters,\n","        beta_schedule=\"squaredcos_cap_v2\",\n","        clip_sample=True,\n","        prediction_type=\"epsilon\"\n","    )\n","    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-6)\n","    ema = EMAModel(parameters=model.parameters(), power=0.75)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = 0.0\n","        for obs_batch, joint_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            obs_batch, joint_batch = obs_batch.to(device).squeeze(1).float(), joint_batch.to(device).squeeze(1).float()\n","\n","            noise = torch.randn_like(joint_batch)\n","            timesteps = torch.randint(0, num_diffusion_iters, (joint_batch.size(0),), device=device)\n","            noisy_joints = noise_scheduler.add_noise(joint_batch, noise, timesteps)\n","\n","            global_cond = obs_batch.reshape(obs_batch.size(0), -1)\n","            noise_pred = model(noisy_joints, timesteps, global_cond)\n","\n","            loss = compute_loss(noise_pred, noise)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            ema.step(model.parameters())\n","\n","            epoch_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n","\n","        # Validation Loop\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                obs_windows, joint_windows = batch\n","                obs_windows = obs_windows.squeeze(1).float().to(device)\n","                joint_windows = joint_windows.squeeze(1).float().to(device)\n","                B = obs_windows.shape[0]\n","                global_cond = obs_windows.reshape(B, -1)\n","                noise = torch.randn_like(joint_windows)\n","                timesteps = torch.randint(0, num_diffusion_iters, (B,), device=device).long()\n","                noisy_joints = noise_scheduler.add_noise(joint_windows, noise, timesteps)\n","                noise_pred = model(noisy_joints, timesteps, global_cond)\n","                val_loss += nn.functional.mse_loss(noise_pred, noise).item()\n","\n","        val_loss /= len(val_loader)\n","        print(f\"Validation Loss: {val_loss}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            no_improve_epochs = 0\n","        else:\n","            no_improve_epochs += 1\n","            if no_improve_epochs >= early_stop_patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","        # Apply EMA weights for evaluation\n","        ema.copy_to(model.parameters())\n","\n","    ema.copy_to(model.parameters())\n","    return model\n"],"metadata":{"id":"NKRWFtY-fD51","executionInfo":{"status":"ok","timestamp":1738075270986,"user_tz":300,"elapsed":148,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["import h5py\n","import csv\n","\n","train_loader, val_loader, test_loader = create_base_data_loaders(\n","        config=config,\n","        train_subjects=[f'subject_{x}' for x in range(2,14)],\n","        test_subjects=['subject_1'],\n","        window_length=100,\n","        window_overlap=75,\n","        batch_size=256\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtRSKZoCYdbT","executionInfo":{"status":"ok","timestamp":1738075354490,"user_tz":300,"elapsed":80867,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}},"outputId":"da7e3efd-63f0-411d-d3e9-cba1fd81c068"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Sharded data found at /content/datasets/dataset_wl100_ol75_train_2_3_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_1. Skipping resharding.\n"]}]},{"cell_type":"code","source":["\n","trained_model = train_diffusion_policy(\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    test_loader=test_loader,\n","    device=device,\n","    num_diffusion_iters=100,\n","    num_epochs=50,\n","    lr=1e-4\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399,"referenced_widgets":["15431e1cb9184ab09ce731c2ccbb5473","45093bc557a34c23b8cd88e3f7f57cd0","e484c0c04f0041dea10083b47a005fa8","a88033e49db24b9aac07cda6a4465307","a99ebb4a050e4d2793bc0ac832810083","4a9f7c0ad50e40ccb6a5c0e3e892f71c","6320e8c4050a40109919371df70106ec","db9cd00ee8b249aea0381ad19acab31c","4d1ed56455ec42e69df053457c19810a","523be5d3f3b347559db922e8059b9327","4e950153bd7f4b74bc9a9c77e3cc06d4"]},"id":"ZS5YsL9MjdAw","outputId":"0ad93ce6-ddd0-4271-f130-94d7db98be8f","executionInfo":{"status":"error","timestamp":1738075358034,"user_tz":300,"elapsed":3557,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["Epoch 1/50:   0%|          | 0/82 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15431e1cb9184ab09ce731c2ccbb5473"}},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 1. Expected size 13 but got size 25 for tensor number 1 in the list.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-29cd6ac73d9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trained_model = train_diffusion_policy(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-43760869ad2b>\u001b[0m in \u001b[0;36mtrain_diffusion_policy\u001b[0;34m(train_loader, val_loader, test_loader, device, num_epochs, lr, num_diffusion_iters, early_stop_patience)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mglobal_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mnoise_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_joints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-ae847bd93b69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, global_cond)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 13 but got size 25 for tensor number 1 in the list."]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.cm import get_cmap\n","\n","# Function to unscale joints\n","def unscale_joints(scaled_data, joint_ranges, joint_names):\n","    unscaled = scaled_data.copy()\n","    for i, ch in enumerate(joint_names):\n","        min_val, max_val = joint_ranges[ch]\n","        unscaled[:, i] = 0.5 * (unscaled[:, i] + 1.0) * (max_val - min_val) + min_val\n","    return unscaled\n","\n","# Joint ranges dictionary\n","joint_ranges = {\n","    'elbow_flex_r': (0, 150),\n","    'arm_flex_r': (-90, 180),\n","    'arm_add_r': (-120, 90),\n","}\n","\n","# Use the trained model\n","model = trained_model\n","model.eval()\n","\n","# Define different numbers of inference steps (must not exceed training timesteps)\n","inference_steps_list = [25, 50, 75, 100]\n","\n","# Grab a batch from the test loader\n","obs_batch, joint_batch = next(iter(test_loader))\n","\n","# Adjust data shape as needed\n","obs_batch = obs_batch.squeeze(1).float().to(device)   # [B, T, obs_channels]\n","joint_batch = joint_batch.squeeze(1).float().to(device)  # [B, T, joint_channels]\n","\n","# Use the first sample for plotting\n","obs_sample = obs_batch[0:1]\n","joint_sample = joint_batch[0:1]\n","B, T, joint_channels = joint_sample.shape\n","\n","# Flatten observation for FiLM conditioning\n","global_cond = obs_sample.reshape(B, -1)\n","\n","# Store predictions for different inference steps\n","all_predictions_unscaled = []\n","all_avg_predictions_unscaled = []\n","\n","# Iterate over different numbers of inference steps\n","for num_inference_steps in inference_steps_list:\n","    # Initialize noise scheduler for current inference steps\n","    noise_scheduler = DDPMScheduler(\n","        num_train_timesteps=100,  # Match training timesteps\n","        beta_schedule=\"squaredcos_cap_v2\",\n","        clip_sample=True,\n","        prediction_type=\"epsilon\"\n","    )\n","    # Subsample timesteps for inference\n","    noise_scheduler.set_timesteps(num_inference_steps)\n","    noise_scheduler.timesteps = noise_scheduler.timesteps.to(device)\n","    noise_scheduler.alphas_cumprod = noise_scheduler.alphas_cumprod.to(device)\n","\n","    predictions = []\n","    for _ in range(20):  # Number of samples per inference step configuration\n","        sample = torch.randn_like(joint_sample)\n","        for step_id in noise_scheduler.timesteps:\n","            t = torch.tensor([step_id], device=noise_scheduler.alphas_cumprod.device)\n","            noise_pred = model(sample, t, global_cond)\n","            step_output = noise_scheduler.step(noise_pred, t, sample)\n","            sample = step_output.prev_sample\n","        predictions.append(sample.detach().cpu().numpy()[0])\n","\n","    # Compute the average prediction\n","    avg_prediction = sum(predictions) / len(predictions)\n","\n","    # Unscale predictions\n","    joint_names = joint_ranges.keys()\n","    predictions_unscaled = [unscale_joints(pred, joint_ranges, joint_names) for pred in predictions]\n","    avg_prediction_unscaled = unscale_joints(avg_prediction, joint_ranges, joint_names)\n","\n","    # Store unscaled predictions\n","    all_predictions_unscaled.append(predictions_unscaled)\n","    all_avg_predictions_unscaled.append(avg_prediction_unscaled)\n","\n","# Unscale the ground truth\n","true_unscaled = unscale_joints(joint_sample.detach().cpu().numpy()[0], joint_ranges, joint_ranges.keys())\n","\n","# Plot each joint channel\n","time_steps = range(T)\n","cmap = get_cmap(\"tab10\")  # Use colormap for different step configurations\n","for c in range(joint_channels):\n","    plt.figure(figsize=(10, 6))\n","\n","    # Plot unscaled ground truth\n","    plt.plot(time_steps, true_unscaled[:, c], label=\"Actual\", color=\"black\", linewidth=2)\n","\n","    # Plot predictions for each inference step configuration\n","    for idx, (predictions_unscaled, avg_prediction_unscaled) in enumerate(zip(all_predictions_unscaled, all_avg_predictions_unscaled)):\n","        # Individual predictions\n","        for pred_unscaled in predictions_unscaled:\n","            plt.plot(\n","                time_steps,\n","                pred_unscaled[:, c],\n","                alpha=0.2,\n","                color=cmap(idx),\n","                label=f\"{inference_steps_list[idx]} Steps\" if pred_unscaled is predictions_unscaled[0] else None\n","            )\n","\n","        # Average prediction\n","        plt.plot(\n","            time_steps,\n","            avg_prediction_unscaled[:, c],\n","            color=cmap(idx),\n","            linewidth=2,\n","            linestyle=\"--\",\n","            label=f\"Avg ({inference_steps_list[idx]} Steps)\"\n","        )\n","\n","    plt.title(f\"Joint Channel {c}\")\n","    plt.xlabel(\"Time Step\")\n","    plt.ylabel(\"Joint Value\")\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"ogXNRwgijgfq","executionInfo":{"status":"aborted","timestamp":1738074882414,"user_tz":300,"elapsed":14,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from sklearn.metrics import mean_squared_error\n","from scipy.stats import pearsonr\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","\n","def calculate_rmse_and_pcc(model, test_loader, noise_scheduler, joint_ranges, num_samples=20, device=\"cpu\"):\n","    model.eval()\n","    noise_scheduler.timesteps = noise_scheduler.timesteps.to(device)\n","    noise_scheduler.alphas_cumprod = noise_scheduler.alphas_cumprod.to(device)\n","\n","    joint_names = list(joint_ranges.keys())\n","    num_joints = len(joint_names)\n","    total_rmse = 0.0\n","    total_pcc = 0.0\n","    num_batches = 0\n","\n","    # To accumulate channel-wise metrics\n","    channel_rmse = np.zeros(num_joints)\n","    channel_pcc = np.zeros(num_joints)\n","    channel_counts = np.zeros(num_joints)\n","\n","    for obs_batch, joint_batch in tqdm(test_loader, desc=\"Processing Batches\"):\n","        # Adjust data shape\n","        obs_batch = obs_batch.squeeze(1).float().to(device)   # [B, T, obs_channels]\n","        joint_batch = joint_batch.squeeze(1).float().to(device)  # [B, T, joint_channels]\n","        B, T, joint_channels = joint_batch.shape\n","\n","        # Collect predictions for the entire batch\n","        all_predictions = []\n","        for _ in range(num_samples):\n","            sample = torch.randn(B, T, joint_channels, device=device)  # Batch of noise\n","            for step_id in noise_scheduler.timesteps:\n","                t = torch.full((B,), step_id.item(), device=device, dtype=torch.long)  # Single timestep for all samples\n","                noise_pred = model(sample, t, obs_batch.reshape(B, -1))\n","                step_output = noise_scheduler.step(noise_pred, step_id, sample)\n","                sample = step_output.prev_sample\n","            all_predictions.append(sample)\n","\n","        # Average predictions over num_samples\n","        avg_predictions = torch.stack(all_predictions).mean(dim=0)  # [B, T, joint_channels]\n","\n","        # Unscale ground truth and predictions\n","        true_unscaled = unscale_joints(joint_batch.cpu().numpy(), joint_ranges, joint_names)\n","        avg_unscaled = unscale_joints(avg_predictions.detach().cpu().numpy(), joint_ranges, joint_names)\n","\n","        # Calculate RMSE and PCC in vectorized form\n","        batch_rmse = np.sqrt(((true_unscaled - avg_unscaled) ** 2).mean(axis=1))  # [B, joint_channels]\n","        batch_pcc = np.array([\n","            [pearsonr(true_unscaled[b, :, c], avg_unscaled[b, :, c])[0] for c in range(joint_channels)]\n","            for b in range(B)\n","        ])  # [B, joint_channels]\n","\n","        # Aggregate metrics\n","        total_rmse += batch_rmse.mean()  # Mean RMSE across batch\n","        total_pcc += batch_pcc.mean()  # Mean PCC across batch\n","\n","        # Channel-wise metrics\n","        channel_rmse += batch_rmse.sum(axis=0)  # Sum over batch\n","        channel_pcc += batch_pcc.sum(axis=0)  # Sum over batch\n","        channel_counts += B  # Accumulate counts\n","\n","        num_batches += 1\n","\n","    # Average channel-wise metrics\n","    avg_channel_rmse = channel_rmse / channel_counts\n","    avg_channel_pcc = channel_pcc / channel_counts\n","\n","    # Total averages\n","    avg_rmse = total_rmse / num_batches\n","    avg_pcc = total_pcc / num_batches\n","\n","    return avg_rmse, avg_pcc, avg_channel_rmse, avg_channel_pcc\n","\n","\n","\n","# Calculate metrics across the test set\n","avg_rmse, avg_pcc, avg_channel_rmse, avg_channel_pcc = calculate_rmse_and_pcc(\n","    model=trained_model,\n","    test_loader=test_loader,\n","    noise_scheduler=noise_scheduler,\n","    joint_ranges=joint_ranges,\n","    num_samples=10,\n","    device=device\n",")\n","\n","# Print total averages\n","print(f\"Total Average RMSE: {avg_rmse:.4f}\")\n","print(f\"Total Average PCC: {avg_pcc:.4f}\")\n","\n","# Print channel-wise averages\n","for i, joint_name in enumerate(joint_ranges.keys()):\n","    print(f\"{joint_name} - RMSE: {avg_channel_rmse[i]:.4f}, PCC: {avg_channel_pcc[i]:.4f}\")\n"],"metadata":{"id":"qmFk_qVGmAfg","executionInfo":{"status":"aborted","timestamp":1738074882415,"user_tz":300,"elapsed":13,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n"],"metadata":{"id":"0gEZYKDE0WFB","executionInfo":{"status":"aborted","timestamp":1738074882416,"user_tz":300,"elapsed":13,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G5DT24RpUkLU","executionInfo":{"status":"aborted","timestamp":1738074882417,"user_tz":300,"elapsed":14,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1GjnFTa8apP0bEfEUTiMGHBbRUwqYI0fh","timestamp":1738025623788},{"file_id":"1f09kwLZ2aAjr8cfLWWi2hAfGbjhEadt2","timestamp":1738003784835},{"file_id":"1OrqOEtMnWVFcSotx3oRVP7Kb6nAp5gKk","timestamp":1737975682110},{"file_id":"1ca6IWhaCNijsFxrjbz9MR7LAKEU5B2hT","timestamp":1729719161382},{"file_id":"1GG-T-9Pn3nWxXXDd3VzbjBUDDVBdquPe","timestamp":1727255073654},{"file_id":"1Odo2lxio-f6aVjKEBW4pGlFhEhRFuhig","timestamp":1726983228854},{"file_id":"1AKDQttSbC8SMdITVTIjya06gkxYOJ_NR","timestamp":1726706052358},{"file_id":"14JnySuUfKLfxc10Cw-McYCVHrbCl8e6x","timestamp":1726654334086},{"file_id":"1E6EBrFXKUIM8p9LD1XD4qYq-FnhvYnPz","timestamp":1726443626748},{"file_id":"1IYYQsxrg4uHErJuoV8MFIy_W67h0kWYl","timestamp":1726192532263},{"file_id":"11wbW9XhB8W7ViaUbAd6at_S5bG1-httV","timestamp":1726140420655},{"file_id":"12OM6Fm5sj9mUIEB3bktkaIeGsmM9Zqax","timestamp":1726107191041},{"file_id":"1Gu2Mego9pAJ1jH7ReLQef58Qo09Crmvu","timestamp":1725923231392},{"file_id":"1rdHb4TuCnnIDmoPaIx_xt1QlqeoqZ3aJ","timestamp":1725867609654},{"file_id":"1zVQFZK4F4nFC3rAsfc5f276kkLKyabA7","timestamp":1725770443175},{"file_id":"1_srYfBgGy8FQIMSohL9HL3Ac4ZZfvPVF","timestamp":1722559818032},{"file_id":"1ueeVtfayoqaNooAbjpcCBoKOWg7UmxAp","timestamp":1722359381849},{"file_id":"1ryl9H3tW6u9DyNInb-iS82rOZ4anjgZt","timestamp":1722295666388},{"file_id":"1lyKGsrpoLMhWE9Qz6A7qFZ5-o3fVuSVw","timestamp":1722291006477},{"file_id":"1JhajboXIAvcWgKNCN4Ljlg-Ebih6rbi3","timestamp":1722268029267},{"file_id":"1-tWEKDHgFp0R-NvBdAJIIFHZKc6185I4","timestamp":1722201240061},{"file_id":"1r91HidleatpLF4x2rdc9DnxWyb-U9exV","timestamp":1722197547794},{"file_id":"1sWKusmF7ocIZanW6vcld-Mr6bzERe-kb","timestamp":1722196228475},{"file_id":"1nzXq_89_RbuU-OR3LFr-idc_Gl8aypPt","timestamp":1722195060257},{"file_id":"1v8w64kwmH2zehSmEaUGsLZqJNEwecL-i","timestamp":1722185530003},{"file_id":"1QuAo2poyCHl3DFfE8Wrj9OnfDzv5HgpR","timestamp":1722181752384},{"file_id":"11qGVlcaE5KShLP8boMFZU_pIAYHJv2N1","timestamp":1722113928372},{"file_id":"1fzC0avZyr80RIwPOYEeAtJIlD_xB0-zv","timestamp":1722100536152}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"15431e1cb9184ab09ce731c2ccbb5473":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45093bc557a34c23b8cd88e3f7f57cd0","IPY_MODEL_e484c0c04f0041dea10083b47a005fa8","IPY_MODEL_a88033e49db24b9aac07cda6a4465307"],"layout":"IPY_MODEL_a99ebb4a050e4d2793bc0ac832810083"}},"45093bc557a34c23b8cd88e3f7f57cd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a9f7c0ad50e40ccb6a5c0e3e892f71c","placeholder":"","style":"IPY_MODEL_6320e8c4050a40109919371df70106ec","value":"Epoch1/50:0%"}},"e484c0c04f0041dea10083b47a005fa8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_db9cd00ee8b249aea0381ad19acab31c","max":82,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d1ed56455ec42e69df053457c19810a","value":0}},"a88033e49db24b9aac07cda6a4465307":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_523be5d3f3b347559db922e8059b9327","placeholder":"","style":"IPY_MODEL_4e950153bd7f4b74bc9a9c77e3cc06d4","value":"0/82[00:01&lt;?,?it/s]"}},"a99ebb4a050e4d2793bc0ac832810083":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a9f7c0ad50e40ccb6a5c0e3e892f71c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6320e8c4050a40109919371df70106ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db9cd00ee8b249aea0381ad19acab31c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d1ed56455ec42e69df053457c19810a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"523be5d3f3b347559db922e8059b9327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e950153bd7f4b74bc9a9c77e3cc06d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}