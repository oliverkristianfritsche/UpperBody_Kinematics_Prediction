{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1304,"status":"ok","timestamp":1729745322920,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"},"user_tz":240},"id":"YkI_GtjaTfqd","outputId":"e902ebec-c6cf-49f7-f77b-c4ee2fb36240"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n"]}],"source":["\n","#mount drive\n","from google.colab import drive\n","drive.mount('/content/MyDrive')\n","import seaborn as sns\n","sns.set_theme(\"paper\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETk_LSRPvM--"},"outputs":[],"source":["# @title Initialize Config\n","\n","import torch\n","import numpy\n","class Config:\n","    def __init__(self, **kwargs):\n","        self.channels_imu_acc = kwargs.get('channels_imu_acc', [])\n","        self.channels_imu_gyr = kwargs.get('channels_imu_gyr', [])\n","        self.channels_joints = kwargs.get('channels_joints', [])\n","        self.channels_emg = kwargs.get('channels_emg', [])\n","        self.seed = kwargs.get('seed', 42)\n","        self.data_folder_name = kwargs.get('data_folder_name', 'default_data_folder_name')\n","        self.dataset_root = kwargs.get('dataset_root', 'default_dataset_root')\n","        self.imu_transforms = kwargs.get('imu_transforms', [])\n","        self.joint_transforms = kwargs.get('joint_transforms', [])\n","        self.emg_transforms = kwargs.get('emg_transforms', [])\n","        self.input_format = kwargs.get('input_format', 'csv')\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","config = Config(\n","    data_folder_name='/content/MyDrive/MyDrive/sd_datacollection_v4/all_subjects_data_final.h5',\n","    dataset_root='/content/datasets',\n","    input_format=\"csv\",\n","    channels_imu_acc=['ACCX1', 'ACCY1', 'ACCZ1','ACCX2', 'ACCY2', 'ACCZ2', 'ACCX3', 'ACCY3', 'ACCZ3', 'ACCX4', 'ACCY4', 'ACCZ4', 'ACCX5', 'ACCY5', 'ACCZ5', 'ACCX6', 'ACCY6', 'ACCZ6'],\n","    channels_imu_gyr=['GYROX1', 'GYROY1', 'GYROZ1', 'GYROX2', 'GYROY2', 'GYROZ2', 'GYROX3', 'GYROY3', 'GYROZ3', 'GYROX4', 'GYROY4', 'GYROZ4', 'GYROX5', 'GYROY5', 'GYROZ5', 'GYROX6', 'GYROY6', 'GYROZ6'],\n","    channels_joints=['elbow_flex_r', 'arm_flex_r', 'arm_add_r'],\n","    channels_emg=['IM EMG4', 'IM EMG5', 'IM EMG6'],\n",")\n","\n","#set seeds\n","torch.manual_seed(config.seed)\n","numpy.random.seed(config.seed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvI4iVwAvg7y"},"outputs":[],"source":["class DataSharder:\n","    def __init__(self, config, split):\n","        self.config = config\n","        self.h5_file_path = config.data_folder_name  # Path to the HDF5 file\n","        self.split = split\n","\n","    def load_data(self, subjects, window_length, window_overlap, dataset_name):\n","        print(f\"Processing subjects: {subjects} with window length: {window_length}, overlap: {window_overlap}\")\n","\n","        self.window_length = window_length\n","        self.window_overlap = window_overlap\n","\n","        # Process the data from the HDF5 file\n","        self._process_and_save_patients_h5(subjects, dataset_name)\n","\n","    def _process_and_save_patients_h5(self, subjects, dataset_name):\n","        # Open the HDF5 file\n","        with h5py.File(self.h5_file_path, 'r') as h5_file:\n","            dataset_folder = os.path.join(self.config.dataset_root, dataset_name, self.split).replace(\"subject\", \"\").replace(\"__\", \"_\")\n","            print(\"Dataset folder:\", dataset_folder)\n","\n","            if os.path.exists(dataset_folder):\n","                print(\"Dataset Exists, Skipping...\")\n","                return\n","\n","            os.makedirs(dataset_folder, exist_ok=True)\n","            print(\"Dataset folder created: \", dataset_folder)\n","\n","            for subject_id in tqdm(subjects, desc=\"Processing subjects\"):\n","                subject_key = subject_id\n","                if subject_key not in h5_file:\n","                    print(f\"Subject {subject_key} not found in the HDF5 file. Skipping.\")\n","                    continue\n","\n","                subject_data = h5_file[subject_key]\n","                session_keys = list(subject_data.keys())  # Sessions for this subject\n","\n","                for session_id in session_keys:\n","                    session_data_group = subject_data[session_id]\n","\n","                    for sessions_speed in session_data_group.keys():\n","                        session_data = session_data_group[sessions_speed]\n","\n","                        # Extract IMU, EMG, and Joint data as numpy arrays\n","                        imu_data, imu_columns = self._extract_channel_data(session_data, self.config.channels_imu_acc + self.config.channels_imu_gyr)\n","                        emg_data, emg_columns = self._extract_channel_data(session_data, self.config.channels_emg)\n","                        joint_data, joint_columns = self._extract_channel_data(session_data, self.config.channels_joints)\n","\n","                        # Shard the data into windows and save each window\n","                        self._save_windowed_data(imu_data, emg_data, joint_data, subject_key, session_id,sessions_speed, dataset_folder, imu_columns, emg_columns, joint_columns)\n","\n","    def _save_windowed_data(self, imu_data, emg_data, joint_data, subject_key, session_id, session_speed, dataset_folder, imu_columns, emg_columns, joint_columns):\n","        window_size = self.window_length\n","        overlap = self.window_overlap\n","        step_size = window_size - overlap\n","\n","        # Path to the CSV log file\n","        csv_file_path = os.path.join(dataset_folder, '..', f\"{self.split}_info.csv\")\n","\n","        # Ensure the folder exists\n","        os.makedirs(dataset_folder, exist_ok=True)\n","\n","        # Prepare CSV log headers (ensure the columns are 'file_name' and 'file_path')\n","        csv_headers = ['file_name', 'file_path']\n","\n","        # Create or append to the CSV log file\n","        file_exists = os.path.isfile(csv_file_path)\n","        with open(csv_file_path, mode='a', newline='') as csv_file:\n","            writer = csv.writer(csv_file)\n","\n","            # Write the headers only if the file is new\n","            if not file_exists:\n","                writer.writerow(csv_headers)\n","\n","            # Determine the total data length based on the minimum length across the data sources\n","            total_data_length = min(imu_data.shape[1], emg_data.shape[1], joint_data.shape[1])\n","\n","            # Adjust the starting point for windows based on total data length\n","            start = 2000 if total_data_length > 4000 else 0\n","\n","            # Ensure that each window across imu_data, emg_data, and joint_data has the same shape before concatenation\n","            for i in range(start, total_data_length - window_size + 1, step_size):\n","                imu_window = imu_data[:, i:i + window_size]\n","                emg_window = emg_data[:, i:i + window_size]\n","                joint_window = joint_data[:, i:i + window_size]\n","\n","                # Check if the window sizes are valid\n","                if imu_window.shape[1] == window_size and emg_window.shape[1] == window_size and joint_window.shape[1] == window_size:\n","                    # Convert windowed data to pandas DataFrame\n","\n","\n","\n","                    imu_df = pd.DataFrame(imu_window.T, columns=imu_columns)\n","                    emg_df = pd.DataFrame(emg_window.T, columns=emg_columns)\n","                    joint_df = pd.DataFrame(joint_window.T, columns=joint_columns)\n","\n","\n","\n","                    # Concatenate the data along the column axis\n","                    combined_df = pd.concat([imu_df, emg_df, joint_df], axis=1)\n","\n","                    # Save the combined windowed data as a CSV file\n","                    file_name = f\"{subject_key}_{session_id}_{session_speed}_win_{i}_ws{window_size}_ol{overlap}.csv\"\n","                    file_path = os.path.join(dataset_folder, file_name)\n","                    combined_df.to_csv(file_path, index=False)\n","\n","                    # Log the file name and path in the CSV (in the correct columns)\n","                    writer.writerow([file_name, file_path])\n","                else:\n","                    print(f\"Skipping window {i} due to mismatched window sizes.\")\n","\n","    def _extract_channel_data(self, session_data, channels):\n","      extracted_data = []\n","      new_column_names = []  # Initialize here\n","\n","      if isinstance(session_data, h5py.Dataset):\n","          if session_data.dtype.names:\n","              # Compound dataset\n","              column_names = session_data.dtype.names\n","              for channel in channels:\n","                  if channel in column_names:\n","                      channel_data = session_data[channel][:]\n","                      channel_data = pd.to_numeric(channel_data, errors='coerce')\n","                      df = pd.DataFrame(channel_data)\n","                      df_interpolated = df.interpolate(method='linear', axis=0, limit_direction='both')\n","                      extracted_data.append(df_interpolated.to_numpy().flatten())\n","                      new_column_names.append(channel)  # Populate here\n","                  else:\n","                      print(f\"Channel {channel} not found in compound dataset.\")\n","          else:\n","              # Simple dataset\n","              column_names = list(session_data.attrs.get('column_names', []))\n","              assert len(column_names) > 0, \"column_names not found in dataset attributes\"\n","              for channel in channels:\n","                  if channel in column_names:\n","                      col_idx = column_names.index(channel)\n","                      channel_data = session_data[:, col_idx]\n","                      channel_data = pd.to_numeric(channel_data, errors='coerce')\n","                      df = pd.DataFrame(channel_data)\n","                      df_interpolated = df.interpolate(method='linear', axis=0, limit_direction='both')\n","                      extracted_data.append(df_interpolated.to_numpy().flatten())\n","                      new_column_names.append(channel)\n","                  else:\n","                      print(f\"Channel {channel} not found in session data.\")\n","\n","      return np.array(extracted_data), new_column_names\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCr-VlFC8Nu7"},"outputs":[],"source":["# @title Dataset creation\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import random_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from torch.utils.data import ConcatDataset\n","import random\n","from torch.utils.data import TensorDataset\n","\n","class ImuJointPairDataset(Dataset):\n","    def __init__(self, config, subjects, window_length, window_overlap, split='train', dataset_train_name='train', dataset_test_name='test'):\n","        self.config = config\n","        self.split = split\n","        self.subjects = subjects\n","        self.window_length = window_length\n","        self.window_overlap = window_overlap if split == 'train' else 0\n","        self.input_format = config.input_format\n","        self.channels_imu_acc = config.channels_imu_acc\n","        self.channels_imu_gyr = config.channels_imu_gyr\n","        self.channels_joints = config.channels_joints\n","        self.channels_emg = config.channels_emg\n","\n","        # Convert the list of subjects to a string that is path-safe\n","        subjects_str = \"_\".join(map(str, subjects)).replace('subject', '').replace('__', '_')\n","\n","        # Use dataset_train_name or dataset_test_name based on split\n","        if split == 'train':\n","            dataset_name = f\"dataset_wl{self.window_length}_ol{self.window_overlap}_train{subjects_str}\"\n","        else:\n","            dataset_name = f\"dataset_wl{self.window_length}_ol{self.window_overlap}_test{subjects_str}\"\n","\n","        self.dataset_name = dataset_name\n","\n","        # Define the root directory based on dataset name\n","        self.root_dir = os.path.join(self.config.dataset_root, self.dataset_name)\n","\n","        # Ensure sharded data exists, if not, reshard\n","        self.ensure_resharded(subjects, dataset_train_name if split == 'train' else dataset_test_name)\n","\n","        info_path = os.path.join(self.root_dir, f\"{split}_info.csv\")\n","        self.data = pd.read_csv(info_path)\n","\n","    def ensure_resharded(self, subjects, dataset_name):\n","        if not os.path.exists(self.root_dir):\n","            print(f\"Sharded data not found at {self.root_dir}. Resharding...\")\n","            data_sharder = DataSharder(self.config,self.split)\n","            # Pass dynamic parameters to sharder\n","            data_sharder.load_data(subjects, window_length=self.window_length, window_overlap=self.window_overlap, dataset_name=self.dataset_name)\n","        else:\n","            print(f\"Sharded data found at {self.root_dir}. Skipping resharding.\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        file_path = os.path.join(self.root_dir,self.split, self.data.iloc[idx, 0])\n","\n","        if self.input_format == \"csv\":\n","            combined_data = pd.read_csv(file_path)\n","        else:\n","            raise ValueError(\"Unsupported input format: {}\".format(self.input_format))\n","\n","        imu_data_acc, imu_data_gyr, joint_data, emg_data = self._extract_and_transform(combined_data)\n","        return imu_data_acc, imu_data_gyr, joint_data, emg_data\n","\n","    def _extract_and_transform(self, combined_data):\n","        imu_data_acc = self._extract_channels(combined_data, self.channels_imu_acc)\n","        imu_data_gyr = self._extract_channels(combined_data, self.channels_imu_gyr)\n","        joint_data = self._extract_channels(combined_data, self.channels_joints)\n","        emg_data = self._extract_channels(combined_data, self.channels_emg)\n","\n","        imu_data_acc = self.apply_transforms(imu_data_acc, self.config.imu_transforms)\n","        imu_data_gyr = self.apply_transforms(imu_data_gyr, self.config.imu_transforms)\n","        joint_data = self.apply_transforms(joint_data, self.config.joint_transforms)\n","        emg_data = self.apply_transforms(emg_data, self.config.emg_transforms)\n","\n","        return imu_data_acc, imu_data_gyr, joint_data, emg_data\n","\n","    def _extract_channels(self, combined_data, channels):\n","        return combined_data[channels].values if self.input_format == \"csv\" else combined_data[:, channels]\n","\n","    def apply_transforms(self, data, transforms):\n","        for transform in transforms:\n","            data = transform(data)\n","        return torch.tensor(data, dtype=torch.float32)\n","\n","def create_base_data_loaders(\n","    config,\n","    train_subjects,\n","    test_subjects,\n","    window_length=100,\n","    window_overlap=75,\n","    batch_size=64,\n","    dataset_train_name='train',\n","    dataset_test_name='test'\n","):\n","    # Create datasets with explicit parameters\n","    train_dataset = ImuJointPairDataset(\n","        config=config,\n","        subjects=train_subjects,\n","        window_length=window_length,\n","        window_overlap=window_overlap,\n","        split='train',\n","        dataset_train_name=dataset_train_name\n","    )\n","\n","    test_dataset = ImuJointPairDataset(\n","        config=config,\n","        subjects=test_subjects,\n","        window_length=window_length,\n","        window_overlap=window_overlap,\n","        split='test',\n","        dataset_test_name=dataset_test_name\n","    )\n","\n","    # Split train dataset into training and validation sets\n","    train_size = int(0.9 * len(train_dataset))\n","    val_size = len(train_dataset) - train_size\n","    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAsMzR7bSB5J"},"outputs":[],"source":["# @title Kinematicsnet Architecture\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","from scipy.signal import butter, filtfilt\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","class Encoder_1(nn.Module):\n","    def __init__(self, input_dim, dropout):\n","        super(Encoder_1, self).__init__()\n","        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0)\n","        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0)\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(128, 32)\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out_1, (h_1, _) = self.lstm_1(x)\n","        out_1 = self.dropout_1(out_1)\n","        out_2, (h_2, _) = self.lstm_2(out_1)\n","        out_2 = self.dropout_2(out_2)\n","        return out_2, (h_1, h_2)\n","\n","class Encoder_2(nn.Module):\n","    def __init__(self, input_dim, dropout):\n","        super(Encoder_2, self).__init__()\n","        self.gru_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0)\n","        self.gru_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0)\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(128, 32)\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out_1, h_1 = self.gru_1(x)\n","        out_1 = self.dropout_1(out_1)\n","        out_2, h_2 = self.gru_2(out_1)\n","        out_2 = self.dropout_2(out_2)\n","        return out_2, (h_1, h_2)\n","\n","\n","class GatingModule(nn.Module):\n","    def __init__(self, input_size):\n","        super(GatingModule, self).__init__()\n","        self.gate = nn.Sequential(\n","            nn.Linear(2*input_size, input_size),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input1, input2):\n","        # Apply gating mechanism\n","        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n","\n","        # Scale the inputs based on the gate output\n","        gated_input1 = input1 * gate_output\n","        gated_input2 = input2 * (1 - gate_output)\n","\n","        # Combine the gated inputs\n","        output = gated_input1 + gated_input2\n","        return output\n","#variable w needs to be checked for correct value, stand-in value used\n","class teacher(nn.Module):\n","    def __init__(self, input_acc, input_gyr, input_emg, drop_prob=0.25, w=100):\n","        super(teacher, self).__init__()\n","\n","        self.w=w\n","        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n","        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n","        self.encoder_1_emg=Encoder_1(input_emg, drop_prob)\n","\n","        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n","        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n","        self.encoder_2_emg=Encoder_2(input_emg, drop_prob)\n","\n","        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n","        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n","        self.BN_emg= nn.BatchNorm1d(input_emg, affine=False)\n","\n","\n","        self.fc = nn.Linear(2*3*128+128,3)\n","        self.dropout=nn.Dropout(p=.05)\n","\n","        self.gate_1=GatingModule(128)\n","        self.gate_2=GatingModule(128)\n","        self.gate_3=GatingModule(128)\n","\n","        self.fc_kd = nn.Linear(3*128, 2*128)\n","\n","               # Define the gating network\n","        self.weighted_feat = nn.Sequential(\n","            nn.Linear(128, 1),\n","            nn.Sigmoid())\n","\n","        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)\n","        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())\n","        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())\n","\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","\n","\n","    def forward(self, x_acc, x_gyr, x_emg):\n","\n","        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n","        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n","        x_emg_1=x_emg.view(x_emg.size(0)*x_emg.size(1),x_emg.size(-1))\n","\n","        x_acc_1=self.BN_acc(x_acc_1)\n","        x_gyr_1=self.BN_gyr(x_gyr_1)\n","        x_emg_1=self.BN_emg(x_emg_1)\n","\n","        x_acc_2=x_acc_1.view(-1, self.w, x_acc_1.size(-1))\n","        x_gyr_2=x_gyr_1.view(-1, self.w, x_gyr_1.size(-1))\n","        x_emg_2=x_emg_1.view(-1, self.w, x_emg_1.size(-1))\n","\n","        # Pass through Encoder 1 for each modality and capture hidden states\n","        x_acc_1, (h_acc_1, _) = self.encoder_1_acc(x_acc_2)\n","        x_gyr_1, (h_gyr_1, _) = self.encoder_1_gyr(x_gyr_2)\n","        x_emg_1, (h_emg_1, _) = self.encoder_1_emg(x_emg_2)\n","\n","        # Pass through Encoder 2 for each modality and capture hidden states\n","        x_acc_2, (h_acc_2, _) = self.encoder_2_acc(x_acc_2)\n","        x_gyr_2, (h_gyr_2, _) = self.encoder_2_gyr(x_gyr_2)\n","        x_emg_2, (h_emg_2, _) = self.encoder_2_emg(x_emg_2)\n","\n","        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n","        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n","        # x_emg=torch.cat((x_emg_1,x_emg_2),dim=-1)\n","\n","        x_acc=self.gate_1(x_acc_1,x_acc_2)\n","        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n","        x_emg=self.gate_3(x_emg_1,x_emg_2)\n","\n","        x=torch.cat((x_acc,x_gyr,x_emg),dim=-1)\n","        x_kd=self.fc_kd(x)\n","\n","\n","        out_1, attn_output_weights=self.attention(x,x,x)\n","\n","        gating_weights = self.gating_net(x)\n","        out_2=gating_weights*x\n","\n","        weights_1 = self.weighted_feat(x[:,:,0:128])\n","        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n","        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])\n","        x_1=weights_1*x[:,:,0:128]\n","        x_2=weights_2*x[:,:,128:2*128]\n","        x_3=weights_3*x[:,:,2*128:3*128]\n","        out_3=x_1+x_2+x_3\n","\n","        out=torch.cat((out_1,out_2,out_3),dim=-1)\n","\n","        gating_weights_1 = self.gating_net_1(out)\n","        out=gating_weights_1*out\n","\n","        out=self.fc(out)\n","\n","        #print(out.shape)\n","        return out, x_kd, (h_acc_1, h_acc_2, h_gyr_1, h_gyr_2, h_emg_1, h_emg_2)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmdCuEysPimX"},"outputs":[],"source":["# @title Loss Functions\n","import statistics\n","\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super(RMSELoss, self).__init__()\n","    def forward(self, output, target):\n","        loss = torch.sqrt(torch.mean((output - target) ** 2))\n","        return loss\n","\n","#prediction function\n","def RMSE_prediction(yhat_4,test_y, output_dim,print_losses=True):\n","\n","  s1=yhat_4.shape[0]*yhat_4.shape[1]\n","\n","  test_o=test_y.reshape((s1,output_dim))\n","  yhat=yhat_4.reshape((s1,output_dim))\n","\n","\n","\n","\n","  y_1_no=yhat[:,0]\n","  y_2_no=yhat[:,1]\n","  y_3_no=yhat[:,2]\n","\n","  y_1=y_1_no\n","  y_2=y_2_no\n","  y_3=y_3_no\n","\n","\n","  y_test_1=test_o[:,0]\n","  y_test_2=test_o[:,1]\n","  y_test_3=test_o[:,2]\n","\n","\n","\n","  cutoff=6\n","  fs=200\n","  order=4\n","\n","  nyq = 0.5 * fs\n","  ## filtering data ##\n","  def butter_lowpass_filter(data, cutoff, fs, order):\n","      normal_cutoff = cutoff / nyq\n","      # Get the filter coefficients\n","      b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","      y = filtfilt(b, a, data)\n","      return y\n","\n","\n","\n","  Z_1=y_1\n","  Z_2=y_2\n","  Z_3=y_3\n","\n","\n","\n","  ###calculate RMSE\n","\n","  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1))))\n","  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2))))\n","  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3))))\n","\n","\n","\n","\n","\n","  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n","  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n","  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n","\n","\n","\n","\n","              ### Correlation ###\n","  p=np.array([p_1,p_2,p_3])\n","  #,p_4,p_5,p_6,p_7])\n","\n","\n","\n","\n","      #### Mean and standard deviation ####\n","\n","  rmse=np.array([rmse_1,rmse_2,rmse_3])\n","  #,rmse_4,rmse_5,rmse_6,rmse_7])\n","\n","      #### Mean and standard deviation ####\n","  m=statistics.mean(rmse)\n","  SD=statistics.stdev(rmse)\n","\n","\n","  m_c=statistics.mean(p)\n","  SD_c=statistics.stdev(p)\n","\n","\n","  if print_losses:\n","    print(rmse_1)\n","    print(rmse_2)\n","    print(rmse_3)\n","    print(\"\\n\")\n","    print(p_1)\n","    print(p_2)\n","    print(p_3)\n","    print('Mean: %.3f' % m,'+/- %.3f' %SD)\n","    print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n","\n","  return rmse, p, Z_1,Z_2,Z_3\n","  #,Z_4,Z_5,Z_6,Z_7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yz_ioT_kTHzC"},"outputs":[],"source":["\n","\n","\n","\n","\n","# @title Model Utils\n","\n","# Evaluation function\n","def evaluate_model(device, model, loader, criterion):\n","    \"\"\"Runs evaluation on the validation or test set.\"\"\"\n","    model.eval()\n","    total_loss = 0.0\n","    total_pcc = np.zeros(len(config.channels_joints))\n","    total_rmse = np.zeros(len(config.channels_joints))\n","\n","    with torch.no_grad():\n","        for i, (data_acc, data_gyr, target, data_EMG) in enumerate(loader):\n","            output= model(data_acc.to(device).float(), data_gyr.to(device).float(), data_EMG.to(device).float())\n","\n","            if isinstance(model, teacher):\n","                output,knowledge_distillation,_ = output\n","                loss = criterion(output, target.to(device).float())\n","            else:\n","                loss = criterion(output, target.to(device).float())\n","\n","            batch_rmse, batch_pcc, _, _, _ = RMSE_prediction(output.detach().cpu().numpy(), target.detach().cpu().numpy(), len(config.channels_joints), print_losses=False)\n","            total_loss += loss.item()\n","            total_pcc += batch_pcc\n","            total_rmse += batch_rmse\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_pcc = total_pcc / len(loader)\n","    avg_rmse = total_rmse / len(loader)\n","\n","    return avg_loss, avg_pcc, avg_rmse\n","\n","\n","\n","def save_checkpoint(model, optimizer, epoch, filename, train_loss, val_loss, test_loss=None,\n","                    channelwise_metrics=None, history=None, curriculum_schedule=None):\n","\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'train_loss': train_loss,\n","        'val_loss': val_loss,\n","        'train_channelwise_metrics': channelwise_metrics['train'],\n","        'val_channelwise_metrics': channelwise_metrics['val'],\n","    }\n","\n","    if test_loss is not None:\n","        checkpoint['test_loss'] = test_loss\n","        checkpoint['test_channelwise_metrics'] = channelwise_metrics['test']\n","\n","    # Save the history (losses, PCCs, RMSEs, channel-wise metrics)\n","    if history:\n","        checkpoint['history'] = history\n","\n","    # Save curriculum schedule\n","    if curriculum_schedule:\n","        checkpoint['curriculum_schedule'] = curriculum_schedule\n","\n","    torch.save(checkpoint, filename)\n","    print(f\"Checkpoint saved for epoch {epoch + 1}\")\n","\n","\n","\n","def train_teacher(device, train_loader, val_loader, test_loader, learn_rate, epochs, model, filename, loss_function, optimizer=None, l1_lambda=None, train_from_last_epoch=False, curriculum_loader=None):\n","    model.to(device)\n","    criterion = loss_function\n","\n","    if optimizer is None:\n","        # Create a default Adam optimizer if none is passed\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n","\n","    train_losses = []\n","    val_losses = []\n","    test_losses = []\n","\n","    train_pccs = []\n","    val_pccs = []\n","    test_pccs = []\n","\n","    train_rmses = []\n","    val_rmses = []\n","    test_rmses = []\n","\n","    train_pccs_channelwise = []\n","    val_pccs_channelwise = []\n","    test_pccs_channelwise = []\n","\n","    train_rmses_channelwise = []\n","    val_rmses_channelwise = []\n","    test_rmses_channelwise = []\n","\n","    # Check for existing checkpoint to resume training\n","    last_epoch = 0\n","    checkpoint_path = f\"/content/MyDrive/MyDrive/models/{filename}/\"\n","\n","    if train_from_last_epoch and os.path.exists(checkpoint_path):\n","        # Scan for the latest saved checkpoint\n","        checkpoints = [f for f in os.listdir(checkpoint_path) if f.endswith('.pth')]\n","        if checkpoints:\n","            checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # Sort by epoch number\n","            latest_checkpoint = checkpoints[-1]\n","            print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n","            checkpoint = torch.load(os.path.join(checkpoint_path, latest_checkpoint))\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","            last_epoch = checkpoint['epoch']  # Continue from the next epoch\n","\n","            # Load the history from checkpoint\n","            train_losses = checkpoint['history']['train_losses']\n","            val_losses = checkpoint['history']['val_losses']\n","            test_losses = checkpoint['history']['test_losses']\n","            train_pccs = checkpoint['history']['train_pccs']\n","            val_pccs = checkpoint['history']['val_pccs']\n","            test_pccs = checkpoint['history']['test_pccs']\n","            train_rmses = checkpoint['history']['train_rmses']\n","            val_rmses = checkpoint['history']['val_rmses']\n","            test_rmses = checkpoint['history']['test_rmses']\n","            train_pccs_channelwise = checkpoint['history']['train_pccs_channelwise']\n","            val_pccs_channelwise = checkpoint['history']['val_pccs_channelwise']\n","            test_pccs_channelwise = checkpoint['history']['test_pccs_channelwise']\n","            train_rmses_channelwise = checkpoint['history']['train_rmses_channelwise']\n","            val_rmses_channelwise = checkpoint['history']['val_rmses_channelwise']\n","            test_rmses_channelwise = checkpoint['history']['test_rmses_channelwise']\n","            if 'curriculum_schedule' in checkpoint:\n","                curriculum_loader.curriculum_schedule = checkpoint['curriculum_schedule']  # Load saved curriculum schedule\n","        else:\n","            print(\"No checkpoints found, starting from scratch.\")\n","    else:\n","        print(\"Starting from scratch.\")\n","\n","    start_time = time.time()\n","    best_val_loss = float('inf')\n","    patience = 10\n","    patience_counter = 0\n","\n","    for epoch in range(last_epoch, epochs):\n","        epoch_start_time = time.time()\n","        model.train()\n","\n","        if curriculum_loader:\n","            curriculum_loader.update_epoch(epoch)\n","            train_loader, val_loader, test_loader = curriculum_loader.get_loaders()\n","        # Track metrics per channel\n","        epoch_train_loss = np.zeros(len(config.channels_joints))\n","        epoch_train_pcc = np.zeros(len(config.channels_joints))\n","        epoch_train_rmse = np.zeros(len(config.channels_joints))\n","\n","        # Use epoch starting from `epoch + 1` since we want to reflect actual starting epoch correctly\n","        for i, (data_acc, data_gyr, target, data_EMG) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\")):\n","            optimizer.zero_grad()\n","\n","            # Ensure inputs are properly sent to device and are of correct type\n","            output = model(data_acc.to(device).float(), data_gyr.to(device).float(), data_EMG.to(device).float())\n","\n","\n","            # Check if output is a tuple, take the first element if true\n","            if isinstance(model, teacher):\n","                output,knowledge_distillation,_ = output\n","                loss = criterion(output, target.to(device).float())\n","\n","            else:\n","                loss = criterion(output, target.to(device).float())\n","\n","            # Compute loss\n","\n","\n","\n","            # Apply L1 regularization if specified\n","            if l1_lambda is not None:\n","                l1_norm = sum(p.abs().sum() for p in model.parameters())\n","                total_loss = loss + l1_lambda * l1_norm\n","            else:\n","                total_loss = loss\n","\n","            # Backpropagate the gradients for total_loss\n","            total_loss.backward()\n","            optimizer.step()\n","\n","            # Detach tensors and move to CPU to prevent issues with gradient computation\n","            batch_rmse, batch_pcc, _, _, _ = RMSE_prediction(output.detach().cpu().numpy(), target.detach().cpu().numpy(), len(config.channels_joints), print_losses=False)\n","\n","            # Accumulate loss, pcc, and rmse without modifying in-place\n","            epoch_train_loss += loss.detach().cpu().numpy()\n","            epoch_train_pcc += batch_pcc\n","            epoch_train_rmse += batch_rmse\n","\n","        avg_train_loss = epoch_train_loss / len(train_loader)\n","        avg_train_pcc = epoch_train_pcc / len(train_loader)\n","        avg_train_rmse = epoch_train_rmse / len(train_loader)\n","\n","        train_losses.append(avg_train_loss)\n","        train_pccs.append(np.mean(avg_train_pcc))  # Overall average PCC\n","        train_rmses.append(np.mean(avg_train_rmse))  # Overall average RMSE\n","\n","        # Save channel-wise metrics\n","        train_pccs_channelwise.append(avg_train_pcc)  # Per channel\n","        train_rmses_channelwise.append(avg_train_rmse)  # Per channel\n","\n","        # Evaluate on validation set every epoch\n","        avg_val_loss, avg_val_pcc, avg_val_rmse = evaluate_model(device, model, val_loader, criterion)\n","        val_losses.append(avg_val_loss)\n","        val_pccs.append(np.mean(avg_val_pcc))  # Overall average PCC\n","        val_rmses.append(np.mean(avg_val_rmse))  # Overall average RMSE\n","\n","        # Save channel-wise metrics\n","        val_pccs_channelwise.append(avg_val_pcc)  # Per channel\n","        val_rmses_channelwise.append(avg_val_rmse)  # Per channel\n","\n","        # Evaluate on test set and checkpoint every epoch\n","        avg_test_loss, avg_test_pcc, avg_test_rmse = evaluate_model(device, model, test_loader, criterion)\n","        test_losses.append(avg_test_loss)\n","        test_pccs.append(np.mean(avg_test_pcc))  # Overall average PCC\n","        test_rmses.append(np.mean(avg_test_rmse))  # Overall average RMSE\n","\n","        # Save channel-wise metrics\n","        test_pccs_channelwise.append(avg_test_pcc)  # Per channel\n","        test_rmses_channelwise.append(avg_test_rmse)  # Per channel\n","\n","        print(f\"Epoch: {epoch + 1}, Training Loss: {np.mean(avg_train_loss):.4f}, Validation Loss: {np.mean(avg_val_loss):.4f}, Test Loss: {np.mean(avg_test_loss):.4f}\")\n","        print(f\"Training RMSE: {np.mean(avg_train_rmse)}, Validation RMSE: {np.mean(avg_val_rmse):.4f}, Test RMSE: {np.mean(avg_test_rmse):.4f}\")\n","        print(f\"Training PCC: {np.mean(avg_train_pcc)}, Validation PCC: {np.mean(avg_val_pcc):.4f}, Test PCC: {np.mean(avg_test_pcc):.4f}\")\n","\n","        # Save checkpoint, including curriculum schedule\n","        if not os.path.exists(checkpoint_path):\n","            os.makedirs(checkpoint_path)\n","\n","        # Save checkpoint with the curriculum schedule\n","        history = {\n","            'train_losses': train_losses,\n","            'val_losses': val_losses,\n","            'test_losses': test_losses,\n","            'train_pccs': train_pccs,\n","            'val_pccs': val_pccs,\n","            'test_pccs': test_pccs,\n","            'train_rmses': train_rmses,\n","            'val_rmses': val_rmses,\n","            'test_rmses': test_rmses,\n","            'train_pccs_channelwise': train_pccs_channelwise,\n","            'val_pccs_channelwise': val_pccs_channelwise,\n","            'test_pccs_channelwise': test_pccs_channelwise,\n","            'train_rmses_channelwise': train_rmses_channelwise,\n","            'val_rmses_channelwise': val_rmses_channelwise,\n","            'test_rmses_channelwise': test_rmses_channelwise\n","        }\n","\n","        save_checkpoint(\n","            model,\n","            optimizer,\n","            epoch,\n","            f\"{checkpoint_path}/{filename}_epoch_{epoch + 1}.pth\",\n","            train_loss=avg_train_loss,\n","            val_loss=avg_val_loss,\n","            test_loss=avg_test_loss,\n","            channelwise_metrics={\n","                'train': {'pcc': avg_train_pcc, 'rmse': avg_train_rmse},\n","                'val': {'pcc': avg_val_pcc, 'rmse': avg_val_rmse},\n","                'test': {'pcc': avg_test_pcc, 'rmse': avg_test_rmse},\n","            },\n","            history=history,  # Save history in the checkpoint\n","            curriculum_schedule=curriculum_loader.curriculum_schedule if curriculum_loader else None # Save curriculum schedule\n","        )\n","\n","        # Early stopping logic\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            torch.save(model.state_dict(), filename)\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","\n","        if patience_counter >= patience:\n","            print(f\"Stopping early after {epoch + 1} epochs\")\n","            break\n","\n","    end_time = time.time()\n","    print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n","\n","    print(f\"loading best model from {filename}\")\n","    model.load_state_dict(torch.load(filename))\n","    model.eval()\n","    return model, train_losses, val_losses, test_losses, train_pccs, val_pccs, test_pccs, train_rmses, val_rmses, test_rmses\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Eul1_WhMTqb"},"outputs":[],"source":["# @title Helper Functions\n","\n","\n","# Function to create the teacher model with defaults from config\n","def create_teacher_model(input_acc, input_gyr, input_emg, base_weights_path=None, drop_prob=0.25, w=100):\n","    model = teacher(input_acc, input_gyr, input_emg, drop_prob=drop_prob, w=w)\n","\n","    if base_weights_path:\n","        # Load the initial weights from the base model\n","        model.load_state_dict(torch.load(base_weights_path))\n","\n","    return model\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyj0qzUqXL93","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729745361625,"user_tz":240,"elapsed":38478,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}},"outputId":"77673424-fd57-4627-e773-98aa741e8876"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running training with subject_1 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_1_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_2_3_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_1. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_1_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 21.1205, Test PCC: 0.7379, Test RMSE: 20.1624\n","Running training with subject_2 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_2_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_3_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_2. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_2_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 22.8090, Test PCC: 0.6512, Test RMSE: 21.6096\n","Running training with subject_3 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_3_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_3. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_3_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 19.8850, Test PCC: 0.7459, Test RMSE: 17.8113\n","Running training with subject_4 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_4_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_4. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_4_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 16.9448, Test PCC: 0.7507, Test RMSE: 16.0845\n","Running training with subject_5 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_5_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_5. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_5_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 19.2594, Test PCC: 0.7255, Test RMSE: 18.5336\n","Running training with subject_6 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_6_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_6. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_6_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 13.0071, Test PCC: 0.8385, Test RMSE: 11.9530\n","Running training with subject_7 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_7_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_7. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_7_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 14.9253, Test PCC: 0.7647, Test RMSE: 14.0933\n","Running training with subject_8 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_8_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_8. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_8_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 13.0277, Test PCC: 0.8008, Test RMSE: 12.3862\n","Running training with subject_9 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_9_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_9. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_9_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 10.6269, Test PCC: 0.8107, Test RMSE: 10.3843\n","Running training with subject_10 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_10_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_10. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_10_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 17.2199, Test PCC: 0.7565, Test RMSE: 16.1232\n","Running training with subject_11 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_11_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_10_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_11. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_11_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 14.5897, Test PCC: 0.7600, Test RMSE: 13.7433\n","Running training with subject_12 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_12_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_10_11_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_12. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_12_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 12.2756, Test PCC: 0.8317, Test RMSE: 11.5106\n","Running training with subject_13 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_13_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_10_11_12. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_13. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_13_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 17.2814, Test PCC: 0.7685, Test RMSE: 16.1865\n","Running training with subject_1 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_1_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_2_3_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_1. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_1_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 21.1205, Test PCC: 0.7379, Test RMSE: 20.1624\n","Running training with subject_2 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_2_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_3_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_2. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_2_wl100_ol75\n","Test Loss: 22.8090, Test PCC: 0.6512, Test RMSE: 21.6096\n","Running training with subject_3 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_3_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_4_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_3. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_3_wl100_ol75\n","Test Loss: 19.8850, Test PCC: 0.7459, Test RMSE: 17.8113\n","Running training with subject_4 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_4_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_5_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_4. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_4_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n","<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n","<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 16.9448, Test PCC: 0.7507, Test RMSE: 16.0845\n","Running training with subject_5 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_5_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_6_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_5. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_5_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 19.2594, Test PCC: 0.7255, Test RMSE: 18.5336\n","Running training with subject_6 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_6_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_7_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_6. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_6_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 13.0071, Test PCC: 0.8385, Test RMSE: 11.9530\n","Running training with subject_7 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_7_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_8_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_7. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_7_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 14.9253, Test PCC: 0.7647, Test RMSE: 14.0933\n","Running training with subject_8 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_8_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_9_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_8. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_8_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 13.0277, Test PCC: 0.8008, Test RMSE: 12.3862\n","Running training with subject_9 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_9_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_10_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_9. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_9_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 10.6269, Test PCC: 0.8107, Test RMSE: 10.3843\n","Running training with subject_10 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_10_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_11_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_10. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_10_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 17.2199, Test PCC: 0.7565, Test RMSE: 16.1232\n","Running training with subject_11 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_11_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_10_12_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_11. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_11_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 14.5897, Test PCC: 0.7600, Test RMSE: 13.7433\n","Running training with subject_12 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_12_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_10_11_13. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_12. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_12_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 12.2756, Test PCC: 0.8317, Test RMSE: 11.5106\n","Running training with subject_13 as the test subject.\n","Model: TeacherModel_RMSELoss_test_subject_13_wl100_ol75\n","Sharded data found at /content/datasets/dataset_wl100_ol75_train_1_2_3_4_5_6_7_8_9_10_11_12. Skipping resharding.\n","Sharded data found at /content/datasets/dataset_wl100_ol0_test_13. Skipping resharding.\n","Running model: TeacherModel_RMSELoss_test_subject_13_wl100_ol75\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-20-44bc30a62883>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(f\"{model_name}\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 17.2814, Test PCC: 0.7685, Test RMSE: 16.1865\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import h5py\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","import csv\n","\n","all_subjects= [f\"subject_{x}\" for x in range(1,14)]\n","input_acc, input_gyr, input_emg = 18,18,3\n","batch_size = 64\n","\n","# Placeholder for storing best RMSEs\n","best_rmse_per_subject = []\n","best_pcc_per_subject = []\n","\n","train_flag = False\n","\n","for test_subject in all_subjects:\n","\n","\n","\n","    print(f\"Running training with {test_subject} as the test subject.\")\n","\n","    # Set up the training subjects (all except the test subject)\n","    train_subjects = [subject for subject in all_subjects if subject != test_subject]\n","\n","    model_name = f'TeacherModel_RMSELoss_test_{test_subject}_wl{100}_ol{75}'\n","    print(f\"Model: {model_name}\")\n","\n","    # Load the model configuration and data loaders\n","    model_config = {\n","        'model': create_teacher_model(input_acc, input_gyr, input_emg, w=100),\n","        'loss': RMSELoss(),\n","        'loaders': create_base_data_loaders(\n","            config=config,\n","            train_subjects=train_subjects,\n","            test_subjects=[test_subject],\n","            window_length=100,\n","            window_overlap=75,\n","            batch_size=batch_size\n","        ),\n","        'epochs': 10,\n","        'use_curriculum': False\n","    }\n","\n","    model = model_config['model']\n","    loss_function = model_config['loss']\n","    epochs = model_config.get(\"epochs\", 10)\n","    device = model_config.get(\"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","    learn_rate = model_config.get(\"learn_rate\", 0.001)\n","    use_curriculum = model_config.get(\"use_curriculum\", False)\n","\n","    optimizer = model_config.get(\"optimizer\", None)\n","    l1_lambda = model_config.get(\"l1_lambda\", None)\n","\n","    print(f\"Running model: {model_name}\")\n","\n","    # Unpack the static loaders tuple (train_loader, val_loader, test_loader)\n","    train_loader, val_loader, test_loader = model_config['loaders']\n","    if train_flag:\n","    # Train the model and save only the best based on validation loss\n","      model, train_losses, val_losses, test_losses, train_pccs, val_pccs, test_pccs, train_rmses, val_rmses, test_rmses = train_teacher(\n","          device=device,\n","          train_loader=train_loader,\n","          val_loader=val_loader,\n","          test_loader=test_loader,\n","          learn_rate=learn_rate,\n","          epochs=epochs,\n","          model=model,\n","          filename=model_name,\n","          loss_function=loss_function,\n","          optimizer=optimizer,\n","          l1_lambda=l1_lambda,\n","          train_from_last_epoch=False\n","      )\n","    else:\n","      #load filename as model\n","      model.load_state_dict(torch.load(f\"{model_name}\"))\n","      model.to(device)\n","      model.eval()\n","\n","     #run model on test set and record result\n","    test_loss, test_pcc, test_rmse = evaluate_model(device, model, test_loader, loss_function)\n","    print(f\"Test Loss: {test_loss:.4f}, Test PCC: {np.mean(test_pcc):.4f}, Test RMSE: {np.mean(test_rmse):.4f}\")\n","    best_rmse_per_subject.append(np.mean(test_rmse))\n","    best_pcc_per_subject.append(np.mean(test_pcc))\n","\n","\n","# Compute the average of the best RMSEs across all subjects\n","\n"]},{"cell_type":"code","source":["\n","average_best_rmse = np.mean(best_rmse_per_subject)\n","average_best_pcc = np.mean(best_pcc_per_subject)\n","print(f\"Average of best RMSEs across all subjects: {average_best_rmse:.4f}\")\n","print(f\"Average of best PCCs across all subjects: {average_best_pcc:.4f}\")\n","print(best_rmse_per_subject)\n","print(best_pcc_per_subject)\n","\n","# subjects = [f'Subject {i+1}' for i in range(len(best_rmse_per_subject))]\n","\n","# print(best_rmse_per_subject)\n","# # Plot a bar chart with subject labels on the x-axis\n","# plt.figure(figsize=(10, 6))\n","# plt.bar(subjects, best_rmse_per_subject, color='blue', edgecolor='black')\n","# plt.title('Best RMSEs for Each Subject')\n","# plt.xlabel('Subjects')\n","# plt.ylabel('Best RMSE')\n","# plt.xticks(rotation=45, ha='right')\n","# plt.grid(True, axis='y')\n","# plt.tight_layout()\n","# plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzvnYVL7WXQd","executionInfo":{"status":"ok","timestamp":1729745361626,"user_tz":240,"elapsed":20,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}},"outputId":"d98d807d-09ce-4367-d7ef-40c88f3b38d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average of best RMSEs across all subjects: 15.4294\n","Average of best PCCs across all subjects: 0.7648\n","[20.162417431672413, 21.609617312749226, 17.81126930316289, 16.08445507287979, 18.53357543547948, 11.95304799079895, 14.093320945898691, 12.38622877995173, 10.384308675924936, 16.123158276081085, 13.7432608405749, 11.510635673999786, 16.186455349127453]\n","[0.7379438233742001, 0.6512409998119354, 0.7458596021991567, 0.7507284215442022, 0.7254871440652823, 0.8385022632535408, 0.764659882815668, 0.800825512123302, 0.8106862904550806, 0.7565179353815933, 0.7599995139604779, 0.8316560668005497, 0.7684594839335279]\n","Average of best RMSEs across all subjects: 15.4294\n","Average of best PCCs across all subjects: 0.7648\n","[20.162417431672413, 21.609617312749226, 17.81126930316289, 16.08445507287979, 18.53357543547948, 11.95304799079895, 14.093320945898691, 12.38622877995173, 10.384308675924936, 16.123158276081085, 13.7432608405749, 11.510635673999786, 16.186455349127453]\n","[0.7379438233742001, 0.6512409998119354, 0.7458596021991567, 0.7507284215442022, 0.7254871440652823, 0.8385022632535408, 0.764659882815668, 0.800825512123302, 0.8106862904550806, 0.7565179353815933, 0.7599995139604779, 0.8316560668005497, 0.7684594839335279]\n"]}]},{"cell_type":"code","source":["import os\n","import zipfile\n","from datetime import datetime\n","\n","notebook_name = 'regression_benchmark'\n","\n","# Create a timestamped folder name based on the notebook name\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","folder_name = f\"{notebook_name}_checkpoints_{timestamp}\"\n","\n","# Make sure the folder exists\n","os.makedirs(folder_name, exist_ok=True)\n","\n","checkpoint_dir = '.'\n","\n","# Zip all checkpoint files and save in the new folder\n","zip_filename = f\"{folder_name}.zip\"\n","with zipfile.ZipFile(zip_filename, 'w') as zipf:\n","    # List files only in the current directory (no subfolders)\n","    for file in os.listdir(checkpoint_dir):\n","        if \"TeacherModel\" in str(file):\n","          file_path = os.path.join(checkpoint_dir, file)\n","          zipf.write(file_path, os.path.relpath(file_path, checkpoint_dir))\n","          print(f\"Checkpoint {file} has been added to the zip file.\")\n","print(f\"All checkpoints have been zipped and saved as {zip_filename}.\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBkUF8_xH1AU","executionInfo":{"status":"ok","timestamp":1729745452019,"user_tz":240,"elapsed":972,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}},"outputId":"c7837d22-a600-4bbe-8afd-3a78bb3dab06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checkpoint TeacherModel_RMSELoss_test_subject_8_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_5_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_13_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_2_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_7_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_3_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_4_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_10_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_6_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_1_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_12_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_9_wl100_ol75 has been added to the zip file.\n","Checkpoint TeacherModel_RMSELoss_test_subject_11_wl100_ol75 has been added to the zip file.\n","All checkpoints have been zipped and saved as regression_benchmark_checkpoints_20241024_045050.zip.\n"]}]},{"cell_type":"code","source":["# Download the zip file to your local machine\n","from google.colab import files\n","files.download(zip_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"hCDIXrKfxAij","executionInfo":{"status":"ok","timestamp":1729745362172,"user_tz":240,"elapsed":40560,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}},"outputId":"4659047d-c8b3-4590-bab9-6aeacb22d718"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_18c2c478-af29-4601-ae46-9b0fe8f8d1f9\", \"regression_benchmark_checkpoints_20241024_044840.zip\", 177340343)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_51b5da87-adc7-4a29-80de-d87affa136da\", \"regression_benchmark_checkpoints_20241024_044921.zip\", 177340343)"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"dRTITfpAzvef"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1ca6IWhaCNijsFxrjbz9MR7LAKEU5B2hT","timestamp":1729719161382},{"file_id":"1GG-T-9Pn3nWxXXDd3VzbjBUDDVBdquPe","timestamp":1727255073654},{"file_id":"1Odo2lxio-f6aVjKEBW4pGlFhEhRFuhig","timestamp":1726983228854},{"file_id":"1AKDQttSbC8SMdITVTIjya06gkxYOJ_NR","timestamp":1726706052358},{"file_id":"14JnySuUfKLfxc10Cw-McYCVHrbCl8e6x","timestamp":1726654334086},{"file_id":"1E6EBrFXKUIM8p9LD1XD4qYq-FnhvYnPz","timestamp":1726443626748},{"file_id":"1IYYQsxrg4uHErJuoV8MFIy_W67h0kWYl","timestamp":1726192532263},{"file_id":"11wbW9XhB8W7ViaUbAd6at_S5bG1-httV","timestamp":1726140420655},{"file_id":"12OM6Fm5sj9mUIEB3bktkaIeGsmM9Zqax","timestamp":1726107191041},{"file_id":"1Gu2Mego9pAJ1jH7ReLQef58Qo09Crmvu","timestamp":1725923231392},{"file_id":"1rdHb4TuCnnIDmoPaIx_xt1QlqeoqZ3aJ","timestamp":1725867609654},{"file_id":"1zVQFZK4F4nFC3rAsfc5f276kkLKyabA7","timestamp":1725770443175},{"file_id":"1_srYfBgGy8FQIMSohL9HL3Ac4ZZfvPVF","timestamp":1722559818032},{"file_id":"1ueeVtfayoqaNooAbjpcCBoKOWg7UmxAp","timestamp":1722359381849},{"file_id":"1ryl9H3tW6u9DyNInb-iS82rOZ4anjgZt","timestamp":1722295666388},{"file_id":"1lyKGsrpoLMhWE9Qz6A7qFZ5-o3fVuSVw","timestamp":1722291006477},{"file_id":"1JhajboXIAvcWgKNCN4Ljlg-Ebih6rbi3","timestamp":1722268029267},{"file_id":"1-tWEKDHgFp0R-NvBdAJIIFHZKc6185I4","timestamp":1722201240061},{"file_id":"1r91HidleatpLF4x2rdc9DnxWyb-U9exV","timestamp":1722197547794},{"file_id":"1sWKusmF7ocIZanW6vcld-Mr6bzERe-kb","timestamp":1722196228475},{"file_id":"1nzXq_89_RbuU-OR3LFr-idc_Gl8aypPt","timestamp":1722195060257},{"file_id":"1v8w64kwmH2zehSmEaUGsLZqJNEwecL-i","timestamp":1722185530003},{"file_id":"1QuAo2poyCHl3DFfE8Wrj9OnfDzv5HgpR","timestamp":1722181752384},{"file_id":"11qGVlcaE5KShLP8boMFZU_pIAYHJv2N1","timestamp":1722113928372},{"file_id":"1fzC0avZyr80RIwPOYEeAtJIlD_xB0-zv","timestamp":1722100536152}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}