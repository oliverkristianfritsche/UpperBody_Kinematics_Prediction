{"cells":[{"cell_type":"code","source":["\n","#mount drive\n","from google.colab import drive\n","drive.mount('/content/MyDrive')\n","import seaborn as sns\n","sns.set_theme(\"paper\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkI_GtjaTfqd","executionInfo":{"status":"ok","timestamp":1725865591871,"user_tz":240,"elapsed":26093,"user":{"displayName":"Oliver Fritsche","userId":"15171898326940313848"}},"outputId":"f4f79934-c895-4bc9-95f8-5830e96b50b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/MyDrive\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from scipy.io import wavfile\n","from sklearn.metrics import mean_squared_error\n","import statistics\n","from scipy.signal import savgol_filter, butter, filtfilt\n","\n","class Config:\n","    def __init__(self, **kwargs):\n","        self.batch_size = kwargs.get('batch_size', 64)\n","        self.epochs = kwargs.get('epochs', 50)\n","        self.lr = kwargs.get('lr', 0.001)\n","        self.channels_imu_acc = kwargs.get('channels_imu_acc', [])\n","        self.channels_imu_acc_test = kwargs.get('channels_imu_acc_test', [])\n","        self.channels_imu_gyr_test = kwargs.get('channels_imu_gyr_test', [])\n","        self.channels_imu_gyr = kwargs.get('channels_imu_gyr', [])\n","        self.channels_joints = kwargs.get('channels_joints', [])\n","        self.channels_emg = kwargs.get('channels_emg', [])\n","        self.seed = kwargs.get('seed', 42)\n","        self.data_folder_name = kwargs.get('data_folder_name', 'default_data_folder_name')\n","        self.dataset_root = kwargs.get('dataset_root', 'default_dataset_root')\n","        self.dataset_train_name = kwargs.get('dataset_train_name', 'train')\n","        self.dataset_test_name = kwargs.get('dataset_test_name', 'test')\n","        self.window_length = kwargs.get('window_length', 100)\n","        self.window_overlap = kwargs.get('window_overlap', 0)\n","        self.imu_transforms = kwargs.get('imu_transforms', [])\n","        self.joint_transforms = kwargs.get('joint_transforms', [])\n","        self.emg_transforms = kwargs.get('emg_transforms', [])\n","        self.input_format = kwargs.get('input_format', 'csv')\n","        self.train_subjects = kwargs.get('train_subjects', [])\n","        self.test_subjects = kwargs.get('test_subjects', [])\n","\n","        self.dataset_name = self.generate_dataset_name()\n","\n","    def generate_dataset_name(self):\n","        name = f\"dataset_wl{self.window_length}_ol{self.window_overlap}_train{self.train_subjects}_test{self.test_subjects}\"\n","        return name"],"metadata":{"id":"FX-hBwHVSd_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImuJointPairDataset(Dataset):\n","    def __init__(self, config, split='train'):\n","        self.config = config\n","        self.split = split\n","        self.input_format = config.input_format\n","        self.channels_imu_acc = config.channels_imu_acc\n","        self.channels_imu_acc_test = config.channels_imu_acc_test\n","        self.channels_imu_gyr = config.channels_imu_gyr\n","        self.channels_imu_gyr_test = config.channels_imu_gyr_test\n","        self.channels_joints = config.channels_joints\n","        self.channels_emg = config.channels_emg\n","\n","        dataset_name = self.config.dataset_name\n","        self.root_dir_train = os.path.join(self.config.dataset_root, dataset_name, self.config.dataset_train_name)\n","        self.root_dir_test = os.path.join(self.config.dataset_root, dataset_name, self.config.dataset_test_name)\n","\n","        train_info_path = os.path.join(self.config.dataset_root, dataset_name, \"train_info.csv\")\n","        test_info_path = os.path.join(self.config.dataset_root, dataset_name, \"test_info.csv\")\n","        self.data = pd.read_csv(train_info_path) if split == 'train' else pd.read_csv(test_info_path)\n","\n","        self.scaler_save_path = os.path.join(self.config.dataset_root, dataset_name, \"scaler.pkl\")\n","        self.scaler = joblib.load(self.scaler_save_path) if os.path.exists(self.scaler_save_path) else None\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if self.split == \"train\":\n","            file_path = os.path.join(self.root_dir_train, self.data.iloc[idx, 0])\n","        else:\n","            file_path = os.path.join(self.root_dir_test, self.data.iloc[idx, 0])\n","\n","        if self.input_format == \"wav\":\n","            combined_data, _ = get_data_from_wav_file(file_path)\n","        elif self.input_format == \"csv\":\n","            combined_data = pd.read_csv(file_path)\n","        else:\n","            raise ValueError(\"Unsupported input format: {}\".format(self.input_format))\n","\n","        imu_data_acc, imu_data_gyr, joint_data, emg_data = self._extract_and_transform(combined_data)\n","        windows = self._apply_windowing(imu_data_acc, imu_data_gyr, joint_data, emg_data, self.config.window_length, self.config.window_overlap)\n","\n","        acc_concat = np.concatenate([w[0] for w in windows], axis=0)\n","        gyr_concat = np.concatenate([w[1] for w in windows], axis=0)\n","        joint_concat = np.concatenate([w[2] for w in windows], axis=0)\n","        emg_concat = np.concatenate([w[3] for w in windows], axis=0)\n","\n","        return acc_concat, gyr_concat, joint_concat, emg_concat\n","\n","    def _extract_and_transform(self, combined_data):\n","        imu_data_acc = self._extract_channels(combined_data, self.channels_imu_acc)\n","        imu_data_gyr = self._extract_channels(combined_data, self.channels_imu_gyr)\n","        joint_data = self._extract_channels(combined_data, self.channels_joints)\n","        emg_data = self._extract_channels(combined_data, self.channels_emg)\n","\n","        combined_data = np.concatenate([imu_data_acc, imu_data_gyr, joint_data, emg_data], axis=1)\n","        scaled_data = combined_data\n","\n","        imu_data_acc = scaled_data[:, :imu_data_acc.shape[1]]\n","        imu_data_gyr = scaled_data[:, imu_data_acc.shape[1]:imu_data_acc.shape[1] + imu_data_gyr.shape[1]]\n","        joint_data = scaled_data[:, imu_data_acc.shape[1] + imu_data_gyr.shape[1]:imu_data_acc.shape[1] + imu_data_gyr.shape[1] + joint_data.shape[1]]\n","        emg_data = scaled_data[:, imu_data_acc.shape[1] + imu_data_gyr.shape[1] + joint_data.shape[1]:]\n","\n","        imu_data_acc = self.apply_transforms(imu_data_acc, self.config.imu_transforms)\n","        imu_data_gyr = self.apply_transforms(imu_data_gyr, self.config.imu_transforms)\n","        joint_data = self.apply_transforms(joint_data, self.config.joint_transforms)\n","        emg_data = self.apply_transforms(emg_data, self.config.emg_transforms)\n","\n","        return imu_data_acc, imu_data_gyr, joint_data, emg_data\n","\n","    def _extract_channels(self, combined_data, channels):\n","        if isinstance(channels, slice):\n","            return combined_data.iloc[:, channels].values if self.input_format == \"csv\" else combined_data[:, channels]\n","        else:\n","            return combined_data[channels].values if self.input_format == \"csv\" else combined_data[:, channels]\n","\n","    def _apply_windowing(self, imu_data_acc, imu_data_gyr, joint_data, emg_data, window_length, window_overlap):\n","        num_samples = imu_data_acc.shape[0]\n","        step = window_length - window_overlap\n","        windows = []\n","\n","        for start in range(0, num_samples - window_length + 1, step):\n","            end = start + window_length\n","            window = (\n","                imu_data_acc[start:end],\n","                imu_data_gyr[start:end],\n","                joint_data[start:end],\n","                emg_data[start:end]\n","            )\n","            windows.append(window)\n","\n","        return windows\n","\n","    def apply_transforms(self, data, transforms):\n","        for transform in transforms:\n","            data = transform(data)\n","        data = torch.tensor(data, dtype=torch.float32)\n","        return data\n"],"metadata":{"id":"xwgqZ7g0Srpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#prediction function\n","def RMSE_prediction(yhat_4,test_y, output_dim):\n","\n","  s1=yhat_4.shape[0]*yhat_4.shape[1]\n","\n","  test_o=test_y.reshape((s1,output_dim))\n","  yhat=yhat_4.reshape((s1,output_dim))\n","\n","\n","\n","\n","  y_1_no=yhat[:,0]\n","  y_2_no=yhat[:,1]\n","  y_3_no=yhat[:,2]\n","\n","  y_1=y_1_no\n","  y_2=y_2_no\n","  y_3=y_3_no\n","\n","\n","  y_test_1=test_o[:,0]\n","  y_test_2=test_o[:,1]\n","  y_test_3=test_o[:,2]\n","\n","\n","\n","  cutoff=6\n","  fs=200\n","  order=4\n","\n","  nyq = 0.5 * fs\n","  ## filtering data ##\n","  def butter_lowpass_filter(data, cutoff, fs, order):\n","      normal_cutoff = cutoff / nyq\n","      # Get the filter coefficients\n","      b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","      y = filtfilt(b, a, data)\n","      return y\n","\n","\n","\n","  Z_1=y_1\n","  Z_2=y_2\n","  Z_3=y_3\n","\n","\n","\n","  ###calculate RMSE\n","\n","  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1))))\n","  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2))))\n","  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3))))\n","\n","\n","  print(rmse_1)\n","  print(rmse_2)\n","  print(rmse_3)\n","\n","\n","  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n","  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n","  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n","\n","\n","  print(\"\\n\")\n","  print(p_1)\n","  print(p_2)\n","  print(p_3)\n","\n","\n","              ### Correlation ###\n","  p=np.array([p_1,p_2,p_3])\n","  #,p_4,p_5,p_6,p_7])\n","\n","\n","\n","\n","      #### Mean and standard deviation ####\n","\n","  rmse=np.array([rmse_1,rmse_2,rmse_3])\n","  #,rmse_4,rmse_5,rmse_6,rmse_7])\n","\n","      #### Mean and standard deviation ####\n","  m=statistics.mean(rmse)\n","  SD=statistics.stdev(rmse)\n","  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n","\n","  m_c=statistics.mean(p)\n","  SD_c=statistics.stdev(p)\n","  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n","\n","\n","\n","  return rmse, p, Z_1,Z_2,Z_3\n","  #,Z_4,Z_5,Z_6,Z_7\n","\n","\n","\n","############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n"],"metadata":{"id":"FUrOvuOnV3TE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","class Encoder_1(nn.Module):\n","    def __init__(self, input_dim, dropout):\n","        super(Encoder_1, self).__init__()\n","        self.lstm_1 = nn.LSTM(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n","        self.lstm_2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n","        self.flatten=nn.Flatten()\n","        self.fc = nn.Linear(128, 32)\n","        self.dropout_1=nn.Dropout(dropout)\n","        self.dropout_2=nn.Dropout(dropout)\n","\n","\n","    def forward(self, x):\n","        out_1, _ = self.lstm_1(x)\n","        out_1=self.dropout_1(out_1)\n","        out_2, _ = self.lstm_2(out_1)\n","        out_2=self.dropout_2(out_2)\n","\n","        return out_2\n","\n","\n","\n","\n","class Encoder_2(nn.Module):\n","    def __init__(self, input_dim, dropout):\n","        super(Encoder_2, self).__init__()\n","        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n","        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n","        self.flatten=nn.Flatten()\n","        self.fc = nn.Linear(128, 32)\n","        self.dropout_1=nn.Dropout(dropout)\n","        self.dropout_2=nn.Dropout(dropout)\n","\n","\n","    def forward(self, x):\n","        out_1, _ = self.lstm_1(x)\n","        out_1=self.dropout_1(out_1)\n","        out_2, _ = self.lstm_2(out_1)\n","        out_2=self.dropout_2(out_2)\n","\n","        return out_2\n","\n","\n","class GatingModule(nn.Module):\n","    def __init__(self, input_size):\n","        super(GatingModule, self).__init__()\n","        self.gate = nn.Sequential(\n","            nn.Linear(2*input_size, input_size),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input1, input2):\n","        # Apply gating mechanism\n","        gate_output = self.gate(torch.cat((input1,input2),dim=-1))\n","\n","        # Scale the inputs based on the gate output\n","        gated_input1 = input1 * gate_output\n","        gated_input2 = input2 * (1 - gate_output)\n","\n","        # Combine the gated inputs\n","        output = gated_input1 + gated_input2\n","        return output\n","#variable w needs to be checked for correct value, stand-in value used\n","class teacher(nn.Module):\n","    def __init__(self, input_acc, input_gyr, input_emg, drop_prob=0.25, w=100):\n","        super(teacher, self).__init__()\n","\n","        self.w=w\n","        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n","        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n","        self.encoder_1_emg=Encoder_1(input_emg, drop_prob)\n","\n","        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n","        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n","        self.encoder_2_emg=Encoder_2(input_emg, drop_prob)\n","\n","        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n","        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n","        self.BN_emg= nn.BatchNorm1d(input_emg, affine=False)\n","\n","\n","        self.fc = nn.Linear(2*3*128+128,3)\n","        self.dropout=nn.Dropout(p=0.05)\n","\n","        self.gate_1=GatingModule(128)\n","        self.gate_2=GatingModule(128)\n","        self.gate_3=GatingModule(128)\n","\n","        self.fc_kd = nn.Linear(3*128, 2*128)\n","\n","               # Define the gating network\n","        self.weighted_feat = nn.Sequential(\n","            nn.Linear(128, 1),\n","            nn.Sigmoid())\n","\n","        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)\n","        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())\n","        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())\n","\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","\n","\n","    def forward(self, x_acc, x_gyr, x_emg):\n","\n","        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n","        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n","        x_emg_1=x_emg.view(x_emg.size(0)*x_emg.size(1),x_emg.size(-1))\n","\n","        x_acc_1=self.BN_acc(x_acc_1)\n","        x_gyr_1=self.BN_gyr(x_gyr_1)\n","        x_emg_1=self.BN_emg(x_emg_1)\n","\n","        x_acc_2=x_acc_1.view(-1, self.w, x_acc_1.size(-1))\n","        x_gyr_2=x_gyr_1.view(-1, self.w, x_gyr_1.size(-1))\n","        x_emg_2=x_emg_1.view(-1, self.w, x_emg_1.size(-1))\n","\n","        x_acc_1=self.encoder_1_acc(x_acc_2)\n","        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n","        x_emg_1=self.encoder_1_emg(x_emg_2)\n","\n","        x_acc_2=self.encoder_2_acc(x_acc_2)\n","        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n","        x_emg_2=self.encoder_2_emg(x_emg_2)\n","\n","        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n","        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n","        # x_emg=torch.cat((x_emg_1,x_emg_2),dim=-1)\n","\n","        x_acc=self.gate_1(x_acc_1,x_acc_2)\n","        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n","        x_emg=self.gate_3(x_emg_1,x_emg_2)\n","\n","        x=torch.cat((x_acc,x_gyr,x_emg),dim=-1)\n","        x_kd=self.fc_kd(x)\n","\n","\n","        out_1, attn_output_weights=self.attention(x,x,x)\n","\n","        gating_weights = self.gating_net(x)\n","        out_2=gating_weights*x\n","\n","        weights_1 = self.weighted_feat(x[:,:,0:128])\n","        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n","        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])\n","        x_1=weights_1*x[:,:,0:128]\n","        x_2=weights_2*x[:,:,128:2*128]\n","        x_3=weights_3*x[:,:,2*128:3*128]\n","        out_3=x_1+x_2+x_3\n","\n","        out=torch.cat((out_1,out_2,out_3),dim=-1)\n","\n","        gating_weights_1 = self.gating_net_1(out)\n","        out=gating_weights_1*out\n","\n","        out=self.fc(out)\n","\n","        #print(out.shape)\n","        return out,x_kd\n","\n","\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super(RMSELoss, self).__init__()\n","    def forward(self, output, target):\n","        loss = torch.sqrt(torch.mean((output - target) ** 2))\n","        return loss\n","\n","\n"],"metadata":{"id":"QM75V9XRVSZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import time\n","from scipy.stats import pearsonr\n","from tqdm import tqdm\n","\n","\n","\n","def save_checkpoint(model, optimizer, epoch, filename, train_loss, val_loss, test_loss=None, channelwise_metrics=None):\n","    \"\"\"Saves the model, optimizer state, and losses (including channel-wise) to a checkpoint.\"\"\"\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'train_loss': train_loss,\n","        'val_loss': val_loss,\n","        'train_channelwise_metrics': channelwise_metrics['train'],\n","        'val_channelwise_metrics': channelwise_metrics['val'],\n","    }\n","    if test_loss is not None:\n","        checkpoint['test_loss'] = test_loss\n","        checkpoint['test_channelwise_metrics'] = channelwise_metrics['test']\n","\n","    torch.save(checkpoint, filename)\n","    print(f\"Checkpoint saved for epoch {epoch + 1}\")\n","\n","\n","def load_checkpoint_if_exists(model, optimizer, filename):\n","    \"\"\"Loads model and optimizer state from a checkpoint if it exists.\"\"\"\n","    if os.path.isfile(filename):\n","        print(f\"Loading checkpoint from {filename}\")\n","        checkpoint = torch.load(filename)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch'] + 1\n","        print(f\"Resuming training from epoch {start_epoch}\")\n","        return start_epoch, checkpoint\n","    return 0, None  # No checkpoint found, start from scratch\n","\n","\n","def evaluate_model(device, model, loader, criterion):\n","    \"\"\"Runs evaluation on the validation or test set.\"\"\"\n","    model.eval()\n","    total_loss = np.zeros(len(config.channels_joints))\n","    total_pcc = np.zeros(len(config.channels_joints))\n","    total_rmse = np.zeros(len(config.channels_joints))\n","\n","    with torch.no_grad():\n","        for i, (data_acc, data_gyr, target, data_EMG) in enumerate(loader):\n","            output, _ = model(data_acc.to(device).float(), data_gyr.to(device).float(), data_EMG.to(device).float())\n","\n","            rmse,pcc = RMSE_prediction(output.detach().cpu().numpy(), target.detach().cpu().numpy(), len(config.channels_joints))\n","            total_loss += np.mean(rmse, axis=0)\n","            total_pcc += np.mean(pcc, axis=0)\n","\n","    avg_loss = total_loss / len(loader)\n","    avg_pcc = total_pcc / len(loader)\n","\n","    return avg_loss, avg_pcc\n","\n","\n","def train_teacher(device, train_loader, val_loader, test_loader, learn_rate, epochs, model, filename):\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","    criterion = RMSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n","\n","    # Load checkpoint if it exists\n","    start_epoch, checkpoint = load_checkpoint_if_exists(model, optimizer, filename)\n","    if checkpoint is not None:\n","        train_losses = checkpoint['train_loss']\n","        val_losses = checkpoint['val_loss']\n","        train_channelwise_metrics = checkpoint['train_channelwise_metrics']\n","        val_channelwise_metrics = checkpoint['val_channelwise_metrics']\n","        best_val_loss = min(val_losses)\n","    else:\n","        train_losses = []\n","        val_losses = []\n","        test_losses = []\n","\n","        train_pccs = []\n","        val_pccs = []\n","        test_pccs = []\n","\n","        train_rmses = []\n","        val_rmses = []\n","        test_rmses = []\n","\n","        best_val_loss = float('inf')\n","\n","    start_time = time.time()\n","    patience = 10\n","    patience_counter = 0\n","\n","    for epoch in range(start_epoch, epochs):\n","        epoch_start_time = time.time()\n","        model.train()\n","\n","        epoch_train_loss = np.zeros(len(config.channels_joints))\n","        epoch_train_pcc = np.zeros(len(config.channels_joints))\n","        epoch_train_rmse = np.zeros(len(config.channels_joints))\n","\n","        for i, (data_acc, data_gyr, target, data_EMG) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\")):\n","            optimizer.zero_grad()\n","            output, _ = model(data_acc.to(device).float(), data_gyr.to(device).float(), data_EMG.to(device).float())\n","            rmse,pcc,_,_,_ = RMSE_prediction(output.detach().cpu().numpy(), target.detach().cpu().numpy(), len(config.channels_joints))\n","            loss = criterion(output, target.to(device).float())\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_train_loss += np.mean(rmse, axis=0)\n","            epoch_train_pcc += np.mean(pcc, axis=0)\n","\n","\n","        avg_train_loss = epoch_train_loss / len(train_loader)\n","        avg_train_pcc = epoch_train_pcc / len(train_loader)\n","\n","        train_losses.append(avg_train_loss)\n","        train_pccs.append(avg_train_pcc)\n","\n","        # Evaluate on validation set every epoch\n","        avg_val_loss, avg_val_pcc = evaluate_model(device, model, val_loader, criterion)\n","        val_losses.append(avg_val_loss)\n","        val_pccs.append(avg_val_pcc)\n","\n","        print(f\"Epoch: {epoch + 1}, Training Loss: {avg_train_loss.mean():.4f}, Validation Loss: {avg_val_loss.mean():.4f}\")\n","\n","        # Checkpoint every 10 epochs, run test set evaluation, and log channel-wise metrics\n","        if (epoch + 1) % 1 == 0:\n","            avg_test_loss, avg_test_pcc, avg_test_rmse = evaluate_model(device, model, test_loader, criterion)\n","            test_losses.append(avg_test_loss)\n","            test_pccs.append(avg_test_pcc)\n","            test_rmses.append(avg_test_rmse)\n","\n","            # Save checkpoint, including channel-wise metrics\n","            save_checkpoint(\n","                model,\n","                optimizer,\n","                epoch,\n","                f\"/content/MyDrive/MyDrive/models/{filename}/{filename}_epoch_{epoch+1}.pth\",\n","                train_loss=avg_train_loss,\n","                val_loss=avg_val_loss,\n","                test_loss=avg_test_loss,\n","                channelwise_metrics={\n","                    'train': {'pcc': avg_train_pcc, 'rmse': avg_train_rmse},\n","                    'val': {'pcc': avg_val_pcc, 'rmse': avg_val_rmse},\n","                    'test': {'pcc': avg_test_pcc, 'rmse': avg_test_rmse},\n","                }\n","            )\n","\n","        # Early stopping logic\n","        if avg_val_loss.mean() < best_val_loss:\n","            best_val_loss = avg_val_loss.mean()\n","            torch.save(model.state_dict(), filename)\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","\n","        if patience_counter >= patience:\n","            print(f\"Stopping early after {epoch + 1} epochs\")\n","            break\n","\n","    end_time = time.time()\n","    print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n","\n","    return model, train_losses, val_losses, test_losses, train_pccs, val_pccs, test_pccs, train_rmses, val_rmses, test_rmses\n"],"metadata":{"id":"Fs1iMAdFVUdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataSharder:\n","    def __init__(self, config, save_h5=False):\n","        self.config = config\n","        self.input_format = config.input_format\n","        self.data_folder_path = config.data_folder_name\n","        self.window_length = int(config.window_length)\n","        self.window_overlap = int(config.window_overlap)\n","        self.save_h5 = save_h5\n","\n","    def load_data(self):\n","        print(f\"Training subjects: {self.config.train_subjects}\")\n","        print(f\"Testing subjects: {self.config.test_subjects}\")\n","\n","        if self.input_format == 'wav':\n","            self._process_and_save_patients_wav(self.config.train_subjects, \"train\")\n","            self._process_and_save_patients_wav(self.config.test_subjects, \"test\")\n","        elif self.input_format == 'csv':\n","            self._process_and_save_patients_csv(self.config.train_subjects, \"train\")\n","            self._process_and_save_patients_csv(self.config.test_subjects, \"test\")\n","        else:\n","            raise ValueError(f\"Unsupported input format: {self.input_format}\")\n","\n","    def _process_and_save_patients_wav(self, patient_id_list, split):\n","        total_data = []\n","        for patient_id in tqdm(patient_id_list, desc=f\"Processing {split} patients\"):\n","            for session_index in tqdm(range(len(self.config.train_subjects)), desc=f\"Processing sessions for {patient_id}\", leave=False):\n","                imu_data, imu_sample_rate = self._load_wav_file(patient_id, session_index, \"IMU\")\n","                joints_data, joints_sample_rate = self._load_wav_file(patient_id, session_index, \"JOINTS\")\n","                emg_data, emg_sample_rate = self._load_wav_file(patient_id, session_index, \"EMG\")\n","\n","                imu_data = self._resample_data(imu_data, imu_sample_rate)\n","                joints_data = self._resample_data(joints_data, joints_sample_rate)\n","                emg_data = self._resample_data(emg_data, emg_sample_rate)\n","\n","                combined_data = torch.cat((imu_data, joints_data, emg_data), dim=1)\n","                total_data.append(combined_data.cpu().numpy())\n","\n","        if self.save_h5:\n","            self._save_to_h5(total_data, split)\n","        else:\n","            for combined_data in total_data:\n","                self._save_windowed_data(combined_data, patient_id, session_index, split)\n","\n","    def _load_wav_file(self, patient_id, session_index, file_type):\n","        file_path = os.path.join(self.data_folder_path, patient_id, f\"run{session_index}_{file_type}.wav\")\n","        data, sample_rate = get_data_from_wav_file(file_path)\n","        return torch.tensor(data, dtype=torch.float32), sample_rate\n","\n","    def _resample_data(self, data, sample_rate):\n","        if sample_rate != self.sample_rate:\n","            data = torch.nn.functional.interpolate(data.unsqueeze(0), size=self.sample_rate, mode='linear').squeeze(0)\n","        return data\n","\n","    def _process_and_save_patients_csv(self, patient_id_list, split):\n","        column_names = None\n","        for patient_id in tqdm(patient_id_list, desc=f\"Processing {split} patients\"):\n","            combined_path = os.path.join(self.data_folder_path, patient_id, \"combined\")\n","            if not os.path.exists(combined_path):\n","                print(f\"Directory {combined_path} does not exist. Skipping patient {patient_id}.\")\n","                continue\n","\n","            patient_files = os.listdir(combined_path)\n","            for session_file in tqdm(patient_files, desc=f\"Processing sessions for {patient_id}\", leave=False):\n","                data = pd.read_csv(os.path.join(combined_path, session_file))\n","                if column_names is None:\n","                    column_names = data.columns.tolist()  # Convert Index to list\n","                data_np = data.to_numpy()\n","                # Pad array to fit columns if necessary\n","                if data_np.shape[1] < len(column_names):\n","                    data_np = np.pad(data_np, ((0, 0), (0, len(column_names) - data_np.shape[1])), mode='constant')\n","                elif data_np.shape[1] > len(column_names):\n","                    # Extend column names to match the data shape\n","                    extra_columns = [f\"extra_{i}\" for i in range(data_np.shape[1] - len(column_names))]\n","                    column_names.extend(extra_columns)\n","\n","                self._save_windowed_data(pd.DataFrame(data_np, columns=column_names), patient_id, session_file.split('.')[0], split, is_csv=True)\n","\n","    def _save_windowed_data(self, data, patient_id, session_id, split, is_csv=False):\n","        dataset_folder = os.path.join(self.config.dataset_root, self.config.dataset_name, self.config.dataset_train_name if split == \"train\" else self.config.dataset_test_name)\n","        os.makedirs(dataset_folder, exist_ok=True)\n","\n","        window_size = self.window_length\n","        overlap = self.window_overlap\n","        step_size = window_size - overlap\n","\n","        data_info_list = []\n","\n","        for i in tqdm(range(0, len(data) - window_size + 1, step_size), desc=f\"Windowing data for {patient_id}_{session_id}\", leave=False):\n","            windowed_data = data.iloc[i:i+window_size] if is_csv else data[i:i+window_size]\n","            if windowed_data.shape[0] < window_size:\n","                continue\n","\n","            windowed_data_np = windowed_data.to_numpy() if is_csv else windowed_data.cpu().numpy()\n","\n","            file_name = f\"{patient_id}_session_{session_id}_window_{i}_ws{window_size}_ol{overlap}.csv\"\n","            file_path = os.path.join(dataset_folder, file_name)\n","            pd.DataFrame(windowed_data_np, columns=data.columns if is_csv else None).to_csv(file_path, index=False)\n","            data_info_list.append({\"file_name\": file_name, \"file_path\": file_path})\n","\n","        data_info_df = pd.DataFrame(data_info_list)\n","        data_info_df.to_csv(os.path.join(self.config.dataset_root, self.config.dataset_name, f\"{split}_info.csv\"), index=False, mode='a', header=not os.path.exists(os.path.join(self.config.dataset_root, self.config.dataset_name, f\"{split}_info.csv\")))\n"],"metadata":{"id":"Pt1wtPxXEpwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","config = Config(\n","    data_folder_name='/content/MyDrive/MyDrive/sd_datacollection',\n","    dataset_root='/content/datasets',\n","    dataset_train_name='train',\n","    dataset_test_name='test',\n","    batch_size=64,\n","    epochs=150,\n","    lr=0.001,\n","    window_length=100,\n","    window_overlap=75,\n","    input_format=\"csv\",\n","    channels_imu_acc=['ACCX1', 'ACCY1', 'ACCZ1', 'ACCX2', 'ACCY2', 'ACCZ2', 'ACCX3', 'ACCY3', 'ACCZ3', 'ACCX4', 'ACCY4', 'ACCZ4', 'ACCX5', 'ACCY5', 'ACCZ5', 'ACCX6', 'ACCY6', 'ACCZ6'],\n","    channels_imu_acc_test=['ACCX1', 'ACCY1', 'ACCZ1', 'ACCX2', 'ACCY2', 'ACCZ2', 'ACCX3', 'ACCY3', 'ACCZ3', 'ACCX4', 'ACCY4', 'ACCZ4', 'ACCX5', 'ACCY5', 'ACCZ5', 'ACCX6', 'ACCY6', 'ACCZ6'],\n","    channels_imu_gyr=['GYROX1', 'GYROY1', 'GYROZ1', 'GYROX2', 'GYROY2', 'GYROZ2', 'GYROX3', 'GYROY3', 'GYROZ3', 'GYROX4', 'GYROY4', 'GYROZ4', 'GYROX5', 'GYROY5', 'GYROZ5', 'GYROX6', 'GYROY6', 'GYROZ6'],\n","    channels_imu_gyr_test=['GYROX1', 'GYROY1', 'GYROZ1', 'GYROX2', 'GYROY2', 'GYROZ2', 'GYROX3', 'GYROY3', 'GYROZ3', 'GYROX4', 'GYROY4', 'GYROZ4', 'GYROX5', 'GYROY5', 'GYROZ5', 'GYROX6', 'GYROY6', 'GYROZ6'],\n","    channels_joints=['elbow_flex_r', 'arm_flex_r', 'arm_add_r'],\n","    channels_emg=['IM EMG4', 'IM EMG5', 'IM EMG6'],\n","    train_subjects=['subject_2','subject_3','subject_4','subject_5','subject_6','subject_7','subject_8', 'subject_9','subject_10', 'subject_11','subject_12','subject_13'],\n","    test_subjects=['subject_1']\n",")\n","\n","\n","\n"],"metadata":{"id":"IG6q3K80St1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reshard = False\n","\n","if reshard:\n","  if not os.path.exists(\"/content/sd_datacollection\"):\n","    #copy over\n","    shutil.copytree(config.data_folder_name, \"/content/sd_datacollection\")\n","    config.data_folder_name = \"/content/sd_datacollection\"\n","  data_sharder = DataSharder(config)\n","  data_sharder.load_data()\n","\n","  #copy new dataset to drive\n","  # shutil.copytree(os.path.join(\"/content/datasets\",config.dataset_name),os.path.join(\"/content/MyDrive/MyDrive/datasets\",config.dataset_name))\n","\n","if not os.path.exists(\"/content/datasets\"):\n","    #copy over\n","    shutil.copytree(os.path.join(\"/content/MyDrive/MyDrive/datasets\",config.dataset_name),os.path.join(\"/content/datasets\",config.dataset_name))\n","\n"],"metadata":{"id":"AunO8oNSE5JQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create datasets\n","train_dataset = ImuJointPairDataset(config, split='train')\n","test_dataset = ImuJointPairDataset(config, split='test')\n","\n","# Setup validation dataset\n","train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [int(0.9 * len(train_dataset)), len(train_dataset) - int(0.9 * len(train_dataset))])\n","\n","# Setup dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n","\n","# Train the model\n","model = teacher(\n","    input_acc=len(config.channels_imu_acc),\n","    input_gyr=len(config.channels_imu_gyr),\n","    input_emg=len(config.channels_emg)\n",")"],"metadata":{"id":"bbTCI35hXeQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","acc,gyro,target,emg=next(iter(train_loader))\n","print(acc.shape)\n","print(gyro.shape)\n","print(target.shape)\n","print(emg.shape)\n","\n","print(acc.dtype)\n","print(gyro.dtype)\n","print(target.dtype)\n","print(emg.dtype)\n"],"metadata":{"id":"a5fIuXYWwast"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = teacher(18, 18, 3)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","\n","#train teacher\n","model, train_losses, val_losses, test_losses, train_pccs, val_pccs, test_pccs, train_rmses, val_rmses, test_rmses = train_teacher(device,train_loader, val_loader, test_loader,config.lr, config.epochs, model, 'new_opensimmodel_first_run_all_subjects')"],"metadata":{"id":"OMm4-m7E0xTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: plot necessary values\n","\n","import matplotlib.pyplot as plt\n","\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(test_losses, label='Test Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Test Loss')\n","plt.legend()\n","plt.show()\n","\n","\n","plt.plot(train_pccs, label='Train PCC')\n","plt.plot(val_pccs, label='Validation PCC')\n","plt.xlabel('Epoch')\n","plt.ylabel('PCC')\n","plt.title('Training and Validation PCC')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(test_pccs, label='Test PCC')\n","plt.xlabel('Epoch')\n","plt.ylabel('PCC')\n","plt.title('Test PCC')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(train_rmses, label='Train RMSE')\n","plt.plot(val_rmses, label='Validation RMSE')\n","plt.xlabel('Epoch')\n","plt.ylabel('RMSE')\n","plt.title('Training and Validation RMSE')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(test_rmses, label='Test RMSE')\n","plt.xlabel('Epoch')\n","plt.ylabel('RMSE')\n","plt.title('Test RMSE')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"9LbyY7bBKI7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class student(nn.Module):\n","    def __init__(self, input_acc, input_gyr, drop_prob=0.25, w=100):\n","        super(student, self).__init__()\n","        self.w=w\n","\n","        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n","        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n","\n","        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n","        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n","\n","        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n","        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n","\n","        self.fc = nn.Linear(2*2*128+128,3)\n","        self.dropout=nn.Dropout(p=0.05)\n","\n","        self.gate_1=GatingModule(128)\n","        self.gate_2=GatingModule(128)\n","\n","               # Define the gating network\n","        self.weighted_feat = nn.Sequential(\n","            nn.Linear(128, 1),\n","            nn.Sigmoid())\n","\n","        self.attention=nn.MultiheadAttention(2*128,4,batch_first=True)\n","        self.gating_net = nn.Sequential(nn.Linear(128*2, 2*128), nn.Sigmoid())\n","        self.gating_net_1 = nn.Sequential(nn.Linear(2*2*128+128, 2*2*128+128), nn.Sigmoid())\n","\n","\n","    def forward(self, x_acc, x_gyr):\n","\n","        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n","        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n","\n","        x_acc_1=self.BN_acc(x_acc_1)\n","        x_gyr_1=self.BN_gyr(x_gyr_1)\n","\n","        x_acc_2=x_acc_1.view(-1, self.w, x_acc_1.size(-1))\n","        x_gyr_2=x_gyr_1.view(-1, self.w, x_gyr_1.size(-1))\n","\n","        x_acc_1=self.encoder_1_acc(x_acc_2)\n","        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n","\n","        x_acc_2=self.encoder_2_acc(x_acc_2)\n","        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n","\n","        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n","        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n","\n","        x_acc=self.gate_1(x_acc_1,x_acc_2)\n","        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n","\n","        x=torch.cat((x_acc,x_gyr),dim=-1)\n","\n","        out_1, attn_output_weights=self.attention(x,x,x)\n","\n","        gating_weights = self.gating_net(x)\n","        out_2=gating_weights*x\n","\n","        weights_1 = self.weighted_feat(x[:,:,0:128])\n","        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n","        x_1=weights_1*x[:,:,0:128]\n","        x_2=weights_2*x[:,:,128:2*128]\n","        out_3=x_1+x_2\n","\n","\n","        out=torch.cat((out_1,out_2,out_3),dim=-1)\n","\n","        gating_weights_1 = self.gating_net_1(out)\n","        out=gating_weights_1*out\n","\n","        out=self.fc(out)\n","\n","        return out\n","\n"],"metadata":{"id":"lFhgvgRHp8w7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_student(device, train_loader, val_loader, learn_rate, epochs, model, filename, imu_indices):\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","    criterion = RMSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n","\n","    train_losses = []\n","    val_losses = []\n","    train_channel_losses = []\n","    val_channel_losses = []\n","    train_pccs = []\n","    val_pccs = []\n","\n","    start_time = time.time()\n","    best_val_loss = float('inf')\n","    patience = 10\n","    patience_counter = 0\n","\n","    for epoch in range(epochs):\n","        epoch_start_time = time.time()\n","        model.train()\n","        epoch_train_loss = 0\n","        epoch_train_channel_loss = np.zeros(len(config.channels_joints))\n","        epoch_val_channel_loss = np.zeros(len(config.channels_joints))\n","        epoch_train_pcc = np.zeros(len(config.channels_joints))\n","        epoch_val_pcc = np.zeros(len(config.channels_joints))\n","\n","        for data_acc, data_gyr, target, data_EMG in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\"):\n","            optimizer.zero_grad()\n","            data_acc_selected = data_acc[:, :, imu_indices].to(device).float()\n","            data_gyr_selected = data_gyr[:, :, imu_indices].to(device).float()\n","            output = model(data_acc_selected, data_gyr_selected)\n","            loss = criterion(output, target.to(device).float())\n","            loss.backward()\n","            optimizer.step()\n","            epoch_train_loss += loss.item()\n","\n","            batch_loss = np.mean((output.detach().cpu().numpy() - target.detach().cpu().numpy())**2, axis=(0, 1))\n","            epoch_train_channel_loss += batch_loss\n","\n","            batch_pcc = compute_pcc(output.detach().cpu().numpy(), target.detach().cpu().numpy())\n","            epoch_train_pcc += batch_pcc\n","\n","        epoch_train_loss /= len(train_loader)\n","        train_losses.append(epoch_train_loss)\n","        train_channel_losses.append(epoch_train_channel_loss / len(train_loader))\n","        train_pccs.append(epoch_train_pcc / len(train_loader))\n","\n","        model.eval()\n","        epoch_val_loss = 0\n","        with torch.no_grad():\n","            for data_acc, data_gyr, target, data_EMG in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} Validation\"):\n","                data_acc_selected = data_acc[:, :, imu_indices].to(device).float()\n","                data_gyr_selected = data_gyr[:, :, imu_indices].to(device).float()\n","                output = model(data_acc_selected, data_gyr_selected)\n","                loss = criterion(output, target.to(device).float())\n","                epoch_val_loss += loss.item()\n","\n","                batch_loss = np.mean((output.detach().cpu().numpy() - target.detach().cpu().numpy())**2, axis=(0, 1))\n","                epoch_val_channel_loss += batch_loss\n","\n","                batch_pcc = compute_pcc(output.detach().cpu().numpy(), target.detach().cpu().numpy())\n","                epoch_val_pcc += batch_pcc\n","\n","        epoch_val_loss /= len(val_loader)\n","        val_losses.append(epoch_val_loss)\n","        val_channel_losses.append(epoch_val_channel_loss / len(val_loader))\n","        val_pccs.append(epoch_val_pcc / len(val_loader))\n","\n","        epoch_end_time = time.time()\n","        epoch_training_time = epoch_end_time - epoch_start_time\n","\n","        print(f\"Epoch: {epoch + 1}, time: {epoch_training_time:.4f}, Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n","\n","        if epoch_val_loss < best_val_loss:\n","            best_val_loss = epoch_val_loss\n","            torch.save(model.state_dict(), filename)\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","\n","        if patience_counter >= patience:\n","            print(f\"Stopping early after {epoch + 1} epochs\")\n","            break\n","\n","    end_time = time.time()\n","    training_time = end_time - start_time\n","    print(f\"Training time: {training_time} seconds\")\n","\n","    return model, train_losses, val_losses, train_channel_losses, val_channel_losses, train_pccs, val_pccs\n"],"metadata":{"id":"YWxMW_2BqiUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load teacher model from checkpoint\n","teacher_cqweheckpoint = \"/content/MyDrive/MyDrive/models/Vanilla_SD_csv_RMSE_gridsearchtraining_smalldataset_fulldata_[1,2,3,7,9,10]/retrained_teacher_model_train_subject_10_subject_2_subject_3_subject_7_subject_9_test_subject_1_epoch_175.pth\"\n","model_teacher = teacher(18, 18,3)\n","model_teacher.load_state_dict(torch.load(teacher_checkpoint)['model_state_dict'])"],"metadata":{"id":"ZQLWPq6pXIfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, loader, device, output_dim=3):\n","    model.eval()\n","    model.to(device)  # Ensure the model is on the correct device\n","    yhat_5 = []\n","    test_target = []\n","\n","    with torch.no_grad():\n","        for i, (data_acc, data_gyr, target, data_EMG) in enumerate(loader):\n","            data_acc, data_gyr, data_EMG = data_acc.to(device).float(), data_gyr.to(device).float(), data_EMG.to(device).float()\n","            target = target.to(device)\n","            output, _ = model(data_acc, data_gyr, data_EMG)\n","            yhat_5.append(output)\n","            test_target.append(target)\n","\n","            # clear memory\n","            del data_acc, data_gyr, target, data_EMG, output\n","            torch.cuda.empty_cache()\n","\n","    yhat_4 = torch.cat(yhat_5, dim=0).detach().cpu().numpy()\n","    test_target = torch.cat(test_target, dim=0).detach().cpu().numpy()\n","    print(yhat_4.shape)\n","\n","    rmse, p, Z_1, Z_2, Z_3 = RMSE_prediction(yhat_4, test_target, output_dim=output_dim)\n","    return np.hstack([rmse, p])\n","\n","# # Ensure that the device is specified\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Call the evaluate_model function\n","# evaluate_model(model_teacher, test_loader, device)"],"metadata":{"id":"ndAu5Dt-Zq-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define your model (you might need to redefine it for each training session to avoid parameter reuse)\n","from scipy.stats import pearsonr\n","def create_student_model(input_size):\n","    return student(input_size, input_size)\n","\n","# IMU indices for each student\n","imu_indices_list = [\n","    [0, 1, 2],              # First IMU (X, Y, Z)\n","    [3, 4, 5],\n","    [0, 1, 2, 3, 4, 5],     # First and second IMUs (X, Y, Z for each)\n","    # [0, 1, 2, 3, 4, 5, 6, 7, 8], # First, second, and third IMUs (X, Y, Z for each)\n","    # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], # Up to sixth IMU\n","]\n","\n","\n","models = []\n","all_train_losses = []\n","all_val_losses = []\n","all_train_channel_losses = []\n","all_val_channel_losses = []\n","all_train_pccs = []\n","all_val_pccs = []\n","\n","# Train each student model with different IMU indices\n","for i, imu_indices in enumerate(imu_indices_list):\n","    filename = f\"student_model_{i+1}.pth\"\n","    model = create_student_model(input_size=len(imu_indices))\n","    model, train_losses, val_losses, train_channel_losses, val_channel_losses, train_pccs, val_pccs = train_student(\n","        device, train_loader, val_loader, config.lr, 40, model, filename, imu_indices\n","    )\n","    models.append(model)\n","    all_train_losses.append(train_losses)\n","    all_val_losses.append(val_losses)\n","    all_train_channel_losses.append(train_channel_losses)\n","    all_val_channel_losses.append(val_channel_losses)\n","    all_train_pccs.append(train_pccs)\n","    all_val_pccs.append(val_pccs)\n"],"metadata":{"id":"mLbd-K-U4Dx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_results(all_train_channel_losses, all_val_channel_losses, all_train_pccs, all_val_pccs, config):\n","    channels = config.channels_joints\n","    num_models = len(all_train_channel_losses)\n","\n","    for model_idx in range(num_models):\n","        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n","        fig.suptitle(f'Model {model_idx + 1} Metrics')\n","\n","        train_channel_losses = all_train_channel_losses[model_idx]\n","        val_channel_losses = all_val_channel_losses[model_idx]\n","        train_pccs = all_train_pccs[model_idx]\n","        val_pccs = all_val_pccs[model_idx]\n","        epochs = len(train_channel_losses)\n","\n","        # Plot RMSE for each channel\n","        for channel_idx, channel in enumerate(channels):\n","            axs[0, 0].plot(range(epochs), [loss[channel_idx] for loss in train_channel_losses], label=f'Train {channel}')\n","            axs[0, 0].plot(range(epochs), [loss[channel_idx] for loss in val_channel_losses], label=f'Val {channel}')\n","\n","        axs[0, 0].set_xlabel('Epochs')\n","        axs[0, 0].set_ylabel('RMSE (degrees)')\n","        axs[0, 0].set_title('Channel-wise RMSE over Epochs')\n","        axs[0, 0].legend()\n","\n","        # Plot PCC for each channel\n","        for channel_idx, channel in enumerate(channels):\n","            axs[0, 1].plot(range(epochs), [pcc[channel_idx] for pcc in train_pccs], label=f'Train {channel}')\n","            axs[0, 1].plot(range(epochs), [pcc[channel_idx] for pcc in val_pccs], label=f'Val {channel}')\n","\n","        axs[0, 1].set_xlabel('Epochs')\n","        axs[0, 1].set_ylabel('PCC')\n","        axs[0, 1].set_title('Channel-wise PCC over Epochs')\n","        axs[0, 1].legend()\n","\n","        # Plot averaged RMSE for train and validation\n","        avg_train_losses = [np.mean([loss[channel_idx] for channel_idx in range(len(channels))]) for loss in train_channel_losses]\n","        avg_val_losses = [np.mean([loss[channel_idx] for channel_idx in range(len(channels))]) for loss in val_channel_losses]\n","\n","        axs[1, 0].plot(range(epochs), avg_train_losses, label='Avg Train')\n","        axs[1, 0].plot(range(epochs), avg_val_losses, label='Avg Val')\n","\n","        axs[1, 0].set_xlabel('Epochs')\n","        axs[1, 0].set_ylabel('Average RMSE (degrees)')\n","        axs[1, 0].set_title('Average RMSE over Epochs')\n","        axs[1, 0].legend()\n","\n","        # Plot averaged PCC for train and validation\n","        avg_train_pccs = [np.mean([pcc[channel_idx] for channel_idx in range(len(channels))]) for pcc in train_pccs]\n","        avg_val_pccs = [np.mean([pcc[channel_idx] for channel_idx in range(len(channels))]) for pcc in val_pccs]\n","\n","        axs[1, 1].plot(range(epochs), avg_train_pccs, label='Avg Train')\n","        axs[1, 1].plot(range(epochs), avg_val_pccs, label='Avg Val')\n","\n","        axs[1, 1].set_xlabel('Epochs')\n","        axs[1, 1].set_ylabel('Average PCC')\n","        axs[1, 1].set_title('Average PCC over Epochs')\n","        axs[1, 1].legend()\n","\n","        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","        plt.show()\n","\n","plot_results(all_train_channel_losses, all_val_channel_losses, all_train_pccs, all_val_pccs, config)"],"metadata":{"id":"kj7aLT3JEDry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def evaluate_model_on_test(model, test_loader, device, imu_indices):\n","    model.eval()\n","    model.to(device)\n","    yhat_5 = []\n","    test_target = []\n","\n","    with torch.no_grad():\n","        for i, (data_acc, data_gyr, target, emg) in enumerate(test_loader):\n","            # Extract only the relevant indices\n","            data_acc = data_acc[:, :, imu_indices].to(device).float()\n","            data_gyr = data_gyr[:, :, imu_indices].to(device).float()\n","            target = target.to(device)\n","            output = model(data_acc, data_gyr)\n","            yhat_5.append(output)\n","            test_target.append(target)\n","\n","            # clear memory\n","            del data_acc, data_gyr, target, output\n","            torch.cuda.empty_cache()\n","\n","    yhat_4 = torch.cat(yhat_5, dim=0).detach().cpu().numpy()\n","    test_target = torch.cat(test_target, dim=0).detach().cpu().numpy()\n","\n","    rmse, p, Z_1, Z_2, Z_3 = RMSE_prediction(yhat_4, test_target, output_dim=3)\n","    return rmse, p\n","\n","# Evaluate each model on the test set\n","all_test_rmse = []\n","all_test_pcc = []\n","\n","for model, imu_indices in zip(models, imu_indices_list):\n","    rmse, pcc = evaluate_model_on_test(model, test_loader, device, imu_indices)\n","    all_test_rmse.append(rmse)\n","    all_test_pcc.append(pcc)\n","\n","# Colors for each model\n","colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n","\n","# Plot the results\n","def plot_comparison_bar_graph(all_test_rmse, all_test_pcc, imu_indices_list):\n","    num_models = len(all_test_rmse)\n","    model_labels = [f\"Model {i+1}\" for i in range(num_models)]\n","\n","    # Bar plot for RMSE\n","    plt.figure(figsize=(12, 6))\n","    plt.bar(model_labels, [np.mean(rmse) for rmse in all_test_rmse], color=colors[:num_models], alpha=0.6, label='RMSE')\n","    plt.xlabel('Models')\n","    plt.ylabel('Average RMSE (degrees)')\n","    plt.ylim(0, 30)\n","    plt.title('Average RMSE for Each Model on Test Set')\n","    plt.legend()\n","    plt.show()\n","\n","    # Bar plot for PCC\n","    plt.figure(figsize=(12, 6))\n","    plt.bar(model_labels, [np.mean(pcc) for pcc in all_test_pcc], color=colors[:num_models], alpha=0.6, label='PCC')\n","    plt.xlabel('Models')\n","    plt.ylabel('Average PCC')\n","    plt.ylim(0, 1)\n","    plt.title('Average PCC for Each Model on Test Set')\n","    plt.legend()\n","    plt.show()\n","\n","plot_comparison_bar_graph(all_test_rmse, all_test_pcc, imu_indices_list)"],"metadata":{"id":"ZauL7qW8e3Nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#variable w needs to be checked for correct value, stand-in value used\n","class teacher(nn.Module):\n","    def __init__(self, input_acc, input_gyr, input_emg, drop_prob=0.25, w=100):\n","        super(teacher, self).__init__()\n","\n","        self.w=w\n","        self.encoder_1_acc=Encoder_1(input_acc, drop_prob)\n","        self.encoder_1_gyr=Encoder_1(input_gyr, drop_prob)\n","        self.encoder_1_emg=Encoder_1(input_emg, drop_prob)\n","\n","        self.encoder_2_acc=Encoder_2(input_acc, drop_prob)\n","        self.encoder_2_gyr=Encoder_2(input_gyr, drop_prob)\n","        self.encoder_2_emg=Encoder_2(input_emg, drop_prob)\n","\n","        self.BN_acc= nn.BatchNorm1d(input_acc, affine=False)\n","        self.BN_gyr= nn.BatchNorm1d(input_gyr, affine=False)\n","        self.BN_emg= nn.BatchNorm1d(input_emg, affine=False)\n","\n","\n","        self.fc = nn.Linear(2*3*128+128,3)\n","        self.dropout=nn.Dropout(p=0.05)\n","\n","        self.gate_1=GatingModule(128)\n","        self.gate_2=GatingModule(128)\n","        self.gate_3=GatingModule(128)\n","\n","        self.fc_kd = nn.Linear(3*128, 2*128)\n","\n","               # Define the gating network\n","        self.weighted_feat = nn.Sequential(\n","            nn.Linear(128, 1),\n","            nn.Sigmoid())\n","\n","        self.attention=nn.MultiheadAttention(3*128,4,batch_first=True)\n","        self.gating_net = nn.Sequential(nn.Linear(128*3, 3*128), nn.Sigmoid())\n","        self.gating_net_1 = nn.Sequential(nn.Linear(2*3*128+128, 2*3*128+128), nn.Sigmoid())\n","\n","        self.pool = nn.MaxPool1d(kernel_size=2)\n","\n","\n","    def forward(self, x_acc, x_gyr, x_emg):\n","\n","        x_acc_1=x_acc.view(x_acc.size(0)*x_acc.size(1),x_acc.size(-1))\n","        x_gyr_1=x_gyr.view(x_gyr.size(0)*x_gyr.size(1),x_gyr.size(-1))\n","        x_emg_1=x_emg.view(x_emg.size(0)*x_emg.size(1),x_emg.size(-1))\n","\n","        x_acc_1=self.BN_acc(x_acc_1)\n","        x_gyr_1=self.BN_gyr(x_gyr_1)\n","        x_emg_1=self.BN_emg(x_emg_1)\n","\n","        x_acc_2=x_acc_1.view(-1, self.w, x_acc_1.size(-1))\n","        x_gyr_2=x_gyr_1.view(-1, self.w, x_gyr_1.size(-1))\n","        x_emg_2=x_emg_1.view(-1, self.w, x_emg_1.size(-1))\n","\n","        x_acc_1=self.encoder_1_acc(x_acc_2)\n","        x_gyr_1=self.encoder_1_gyr(x_gyr_2)\n","        x_emg_1=self.encoder_1_emg(x_emg_2)\n","\n","        x_acc_2=self.encoder_2_acc(x_acc_2)\n","        x_gyr_2=self.encoder_2_gyr(x_gyr_2)\n","        x_emg_2=self.encoder_2_emg(x_emg_2)\n","\n","        # x_acc=torch.cat((x_acc_1,x_acc_2),dim=-1)\n","        # x_gyr=torch.cat((x_gyr_1,x_gyr_2),dim=-1)\n","        # x_emg=torch.cat((x_emg_1,x_emg_2),dim=-1)\n","\n","        x_acc=self.gate_1(x_acc_1,x_acc_2)\n","        x_gyr=self.gate_2(x_gyr_1,x_gyr_2)\n","        x_emg=self.gate_3(x_emg_1,x_emg_2)\n","\n","        x=torch.cat((x_acc,x_gyr,x_emg),dim=-1)\n","        x_kd=self.fc_kd(x)\n","\n","\n","        out_1, attn_output_weights=self.attention(x,x,x)\n","\n","        gating_weights = self.gating_net(x)\n","        out_2=gating_weights*x\n","\n","        weights_1 = self.weighted_feat(x[:,:,0:128])\n","        weights_2 = self.weighted_feat(x[:,:,128:2*128])\n","        weights_3 = self.weighted_feat(x[:,:,2*128:3*128])\n","        x_1=weights_1*x[:,:,0:128]\n","        x_2=weights_2*x[:,:,128:2*128]\n","        x_3=weights_3*x[:,:,2*128:3*128]\n","        out_3=x_1+x_2+x_3\n","\n","        out=torch.cat((out_1,out_2,out_3),dim=-1)\n","\n","        gating_weights_1 = self.gating_net_1(out)\n","        out=gating_weights_1*out\n","\n","        out=self.fc(out)\n","\n","        #print(out.shape)\n","        return out,x_kd\n","\n","class student_KD(nn.Module):\n","    def __init__(self, input_acc, input_gyr, drop_prob=0.25, w=100):\n","        super(student_KD, self).__init__()\n","        self.w = w\n","        self.encoder_1_acc = Encoder_1(input_acc, drop_prob)\n","        self.encoder_1_gyr = Encoder_1(input_gyr, drop_prob)\n","\n","        self.encoder_2_acc = Encoder_2(input_acc, drop_prob)\n","        self.encoder_2_gyr = Encoder_2(input_gyr, drop_prob)\n","\n","        self.BN_acc = nn.BatchNorm1d(input_acc, affine=False)\n","        self.BN_gyr = nn.BatchNorm1d(input_gyr, affine=False)\n","\n","        self.fc_kd = nn.Linear(2 * 128, 2 * 128)\n","        self.fc = nn.Linear(2 * 2 * 128 + 128, 3)\n","        self.dropout = nn.Dropout(p=0.05)\n","\n","        self.gate_1 = GatingModule(128)\n","        self.gate_2 = GatingModule(128)\n","\n","        self.weighted_feat = nn.Sequential(nn.Linear(128, 1), nn.Sigmoid())\n","        self.attention = nn.MultiheadAttention(2 * 128, 4, batch_first=True)\n","        self.gating_net = nn.Sequential(nn.Linear(128 * 2, 2 * 128), nn.Sigmoid())\n","        self.gating_net_1 = nn.Sequential(nn.Linear(2 * 2 * 128 + 128, 2 * 2 * 128 + 128), nn.Sigmoid())\n","\n","    def forward(self, x_acc, x_gyr):\n","        x_acc_1 = x_acc.view(x_acc.size(0) * x_acc.size(1), x_acc.size(-1))\n","        x_gyr_1 = x_gyr.view(x_gyr.size(0) * x_gyr.size(1), x_gyr.size(-1))\n","\n","        x_acc_1 = self.BN_acc(x_acc_1)\n","        x_gyr_1 = self.BN_gyr(x_gyr_1)\n","\n","        x_acc_2 = x_acc_1.view(-1, self.w, x_acc_1.size(-1))\n","        x_gyr_2 = x_gyr_1.view(-1, self.w, x_gyr_1.size(-1))\n","\n","        x_acc_1 = self.encoder_1_acc(x_acc_2)\n","        x_gyr_1 = self.encoder_1_gyr(x_gyr_2)\n","\n","        x_acc_2 = self.encoder_2_acc(x_acc_2)\n","        x_gyr_2 = self.encoder_2_gyr(x_gyr_2)\n","\n","        x_acc = self.gate_1(x_acc_1, x_acc_2)\n","        x_gyr = self.gate_2(x_gyr_1, x_gyr_2)\n","\n","        x = torch.cat((x_acc, x_gyr), dim=-1)\n","        x_KD = self.fc_kd(x)\n","\n","        out_1, attn_output_weights = self.attention(x, x, x)\n","        gating_weights = self.gating_net(x)\n","        out_2 = gating_weights * x\n","\n","        weights_1 = self.weighted_feat(x[:, :, 0:128])\n","        weights_2 = self.weighted_feat(x[:, :, 128:2 * 128])\n","        x_1 = weights_1 * x[:, :, 0:128]\n","        x_2 = weights_2 * x[:, :, 128:2 * 128]\n","        out_3 = x_1 + x_2\n","\n","        out = torch.cat((out_1, out_2, out_3), dim=-1)\n","        gating_weights_1 = self.gating_net_1(out)\n","        out = gating_weights_1 * out\n","\n","        out = self.fc(out)\n","        return out, x_KD"],"metadata":{"id":"U7tkAFG9vMhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def train_student_with_kd(device, alpha, beta, train_loader, val_loader, learn_rate, epochs, student_model, filename, teacher_model, imu_indices):\n","    student_model.to(device)\n","    teacher_model.to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(student_model.parameters(), lr=learn_rate)\n","\n","    train_losses = []\n","    val_losses = []\n","    train_channel_losses = []\n","    val_channel_losses = []\n","    train_pccs = []\n","    val_pccs = []\n","\n","    start_time = time.time()\n","    best_val_loss = float('inf')\n","    patience = 10\n","    patience_counter = 0\n","\n","    for epoch in range(epochs):\n","        epoch_start_time = time.time()\n","        student_model.train()\n","        epoch_train_loss = 0\n","        epoch_train_channel_loss = np.zeros(len(config.channels_joints))\n","        epoch_val_channel_loss = np.zeros(len(config.channels_joints))\n","        epoch_train_pcc = np.zeros(len(config.channels_joints))\n","        epoch_val_pcc = np.zeros(len(config.channels_joints))\n","\n","        for data_acc, data_gyr, target, data_EMG in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} Training\"):\n","            optimizer.zero_grad()\n","            data_acc_selected = data_acc[:, :, imu_indices].to(device).float()\n","            data_gyr_selected = data_gyr[:, :, imu_indices].to(device).float()\n","\n","            student_output, x_student_1 = student_model(data_acc_selected, data_gyr_selected)\n","            with torch.no_grad():\n","                data_acc_device = data_acc.to(device).float()\n","                data_gyr_device = data_gyr.to(device).float()\n","                data_EMG_device = data_EMG.to(device).float()\n","                teacher_output, x_teacher_1 = teacher_model(data_acc_device, data_gyr_device, data_EMG_device)\n","\n","            target_device = target.to(device).float()\n","            loss = criterion(student_output, target_device) + alpha * criterion(student_output, teacher_output) + beta * criterion(x_student_1, x_teacher_1)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_train_loss += loss.item()\n","\n","            batch_loss = np.mean((student_output.detach().cpu().numpy() - target.detach().cpu().numpy())**2, axis=(0, 1))\n","            epoch_train_channel_loss += batch_loss\n","\n","            batch_pcc = compute_pcc(student_output.detach().cpu().numpy(), target.detach().cpu().numpy())\n","            epoch_train_pcc += batch_pcc\n","\n","        epoch_train_loss /= len(train_loader)\n","        train_losses.append(epoch_train_loss)\n","        train_channel_losses.append(epoch_train_channel_loss / len(train_loader))\n","        train_pccs.append(epoch_train_pcc / len(train_loader))\n","\n","        student_model.eval()\n","        epoch_val_loss = 0\n","        with torch.no_grad():\n","            for data_acc, data_gyr, target, data_EMG in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} Validation\"):\n","                data_acc_selected = data_acc[:, :, imu_indices].to(device).float()\n","                data_gyr_selected = data_gyr[:, :, imu_indices].to(device).float()\n","                output, _ = student_model(data_acc_selected, data_gyr_selected)\n","                loss = criterion(output, target.to(device).float())\n","                epoch_val_loss += loss.item()\n","\n","                batch_loss = np.mean((output.detach().cpu().numpy() - target.detach().cpu().numpy())**2, axis=(0, 1))\n","                epoch_val_channel_loss += batch_loss\n","\n","                batch_pcc = compute_pcc(output.detach().cpu().numpy(), target.detach().cpu().numpy())\n","                epoch_val_pcc += batch_pcc\n","\n","        epoch_val_loss /= len(val_loader)\n","        val_losses.append(epoch_val_loss)\n","        val_channel_losses.append(epoch_val_channel_loss / len(val_loader))\n","        val_pccs.append(epoch_val_pcc / len(val_loader))\n","\n","        epoch_end_time = time.time()\n","        epoch_training_time = epoch_end_time - epoch_start_time\n","\n","        print(f\"Epoch: {epoch + 1}, time: {epoch_training_time:.4f}, Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n","\n","        if epoch_val_loss < best_val_loss:\n","            best_val_loss = epoch_val_loss\n","            torch.save(student_model.state_dict(), filename)\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","\n","        if patience_counter >= patience:\n","            print(f\"Stopping early after {epoch + 1} epochs\")\n","            break\n","\n","    end_time = time.time()\n","    training_time = end_time - start_time\n","    print(f\"Training time: {training_time} seconds\")\n","\n","    return student_model, train_losses, val_losses, train_channel_losses, val_channel_losses, train_pccs, val_pccs\n","\n","# IMU indices for each student\n","imu_indices_list = [\n","    [0, 1, 2],# First IMU (X, Y, Z)\n","    [3,4,5],\n","    [0, 1, 2, 3, 4, 5],     # First and second IMUs (X, Y, Z for each)\n","    # [0, 1, 2, 3, 4, 5, 6, 7, 8], # First, second, and third IMUs (X, Y, Z for each)\n","    # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], # Up to sixth IMU\n","]\n","\n","\n","# Load teacher model from checkpoint\n","teacher_checkpoint = \"/content/MyDrive/MyDrive/models/Vanilla_SD_csv_RMSE_gridsearchtraining_smalldataset_fulldata_[1,2,3,7,9,10]/retrained_teacher_model_train_subject_10_subject_2_subject_3_subject_7_subject_9_test_subject_1_epoch_175.pth\"\n","model_teacher = teacher(18, 18,3)\n","model_teacher.load_state_dict(torch.load(teacher_checkpoint)['model_state_dict'])\n","\n","# Store all KD models and their metrics\n","kd_models = []\n","kd_train_losses = []\n","kd_val_losses = []\n","kd_train_channel_losses = []\n","kd_val_channel_losses = []\n","kd_train_pccs = []\n","kd_val_pccs = []\n","\n","# Training hyperparameters for KD\n","alpha = 0.5\n","beta = 0.5\n","\n","def create_student_model_KD(input_size):\n","    return student_KD(input_size, input_size)\n","\n","# Train each student model with different IMU indices using KD\n","for i, imu_indices in enumerate(imu_indices_list):\n","    filename = f\"student_model_kd_{i+1}.pth\"\n","    model = create_student_model_KD(input_size=len(imu_indices))\n","    model, train_losses, val_losses, train_channel_losses, val_channel_losses, train_pccs, val_pccs = train_student_with_kd(\n","        device, alpha, beta, train_loader, val_loader, config.lr, 40, model, filename, model_teacher, imu_indices\n","    )\n","    kd_models.append(model)\n","    kd_train_losses.append(train_losses)\n","    kd_val_losses.append(val_losses)\n","    kd_train_channel_losses.append(train_channel_losses)\n","    kd_val_channel_losses.append(val_channel_losses)\n","    kd_train_pccs.append(train_pccs)\n","    kd_val_pccs.append(val_pccs)\n","\n"],"metadata":{"id":"V9V77Eny7Pio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# temp_checkpoint = \"/content/student_model_1.pth\"\n","# model_temp = student(3,3)\n","# model_temp.load_state_dict(torch.load(temp_checkpoint))\n","\n","# kd_models = [model_temp]\n","\n","def evaluate_model_on_test(model, test_loader, device, imu_indices):\n","    model.eval()\n","    model.to(device)\n","    yhat_5 = []\n","    test_target = []\n","\n","    with torch.no_grad():\n","        for i, (data_acc, data_gyr, target, emg) in enumerate(test_loader):\n","            # Extract only the relevant indices\n","            data_acc = data_acc[:, :, imu_indices].to(device).float()\n","            data_gyr = data_gyr[:, :, imu_indices].to(device).float()\n","            target = target.to(device)\n","\n","            # Unpack the output from the model\n","            output = model(data_acc, data_gyr)\n","\n","            yhat_5.append(output)\n","            test_target.append(target)\n","\n","            # clear memory\n","            del data_acc, data_gyr, target, output\n","            torch.cuda.empty_cache()\n","\n","    yhat_4 = torch.cat(yhat_5, dim=0).detach().cpu().numpy()\n","    test_target = torch.cat(test_target, dim=0).detach().cpu().numpy()\n","\n","    rmse, p, Z_1, Z_2, Z_3 = RMSE_prediction(yhat_4, test_target, output_dim=3)\n","    return rmse, p\n","\n","\n","# Evaluate each model on the test set\n","all_test_rmse_kd = []\n","all_test_pcc_kd = []\n","\n","for model, imu_indices in zip(kd_models, imu_indices_list):\n","    rmse, pcc = evaluate_model_on_test(model, test_loader, device, imu_indices)\n","    all_test_rmse_kd.append(rmse)\n","    all_test_pcc_kd.append(pcc)\n","\n","# Colors for each model\n","colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n","\n","# Plot the results\n","def plot_comparison_bar_graph(all_test_rmse, all_test_pcc, imu_indices_list):\n","    num_models = len(all_test_rmse)\n","    model_labels = [f\"Model {i+1}\" for i in range(num_models)]\n","\n","    # Bar plot for RMSE\n","    plt.figure(figsize=(12, 6))\n","    plt.bar(model_labels, [np.mean(rmse) for rmse in all_test_rmse], color=colors[:num_models], alpha=0.6, label='RMSE')\n","    plt.xlabel('Models')\n","    plt.ylabel('Average RMSE (degrees)')\n","    plt.ylim(0, 30)\n","    plt.title('Average RMSE for Each Model on Test Set')\n","    plt.legend()\n","    plt.show()\n","\n","    # Bar plot for PCC\n","    plt.figure(figsize=(12, 6))\n","    plt.bar(model_labels, [np.mean(pcc) for pcc in all_test_pcc], color=colors[:num_models], alpha=0.6, label='PCC')\n","    plt.xlabel('Models')\n","    plt.ylabel('Average PCC')\n","    plt.ylim(0, 1)\n","    plt.title('Average PCC for Each Model on Test Set')\n","    plt.legend()\n","    plt.show()\n","\n","plot_comparison_bar_graph(all_test_rmse_kd, all_test_pcc_kd, imu_indices_list)\n"],"metadata":{"id":"0mws19-yh2-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.array(kd_train_losses).shape)\n","print(np.array(kd_val_losses).shape)\n","print(np.array(kd_train_channel_losses).shape)\n","print(np.array(kd_val_channel_losses).shape)\n","\n","\n","plot_results(kd_train_channel_losses, kd_val_channel_losses, kd_train_pccs, kd_val_pccs,config)"],"metadata":{"id":"o86u9vOkvvEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#concat RMSE bars and PCC bars combined_rmse = all_test_rmse,all_test_rmse_kd,\n","\n","print(all_test_rmse)\n","print(all_test_rmse_kd)\n","print(all_test_pcc)\n","print(all_test_pcc_kd)\n","\n","process_file\n","\n","\n","\n"],"metadata":{"id":"uJOOAIDYFebv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set_theme(\"paper\")\n","import glob\n","# Butterworth lowpass filter\n","def butter_lowpass_filter(data, cutoff, fs, order):\n","    nyq = 0.5 * fs\n","    normal_cutoff = cutoff / nyq\n","    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","    y = filtfilt(b, a, data)\n","    return y\n","\n","# Prediction function\n","def RMSE_prediction(yhat_4, test_y, output_dim):\n","    s1 = yhat_4.shape[0] * yhat_4.shape[1]\n","    test_o = test_y.reshape((-1, output_dim))\n","    yhat = yhat_4.reshape((-1, output_dim))\n","\n","    y_1_no, y_2_no, y_3_no = yhat[:, 0], yhat[:, 1], yhat[:, 2]\n","    y_test_1, y_test_2, y_test_3 = test_o[:, 0], test_o[:, 1], test_o[:, 2]\n","\n","    cutoff, fs, order = 6, 200, 4\n","\n","    # Filtered predictions\n","    y_1_filtered = butter_lowpass_filter(y_1_no, cutoff, fs, order)\n","    y_2_filtered = butter_lowpass_filter(y_2_no, cutoff, fs, order)\n","    y_3_filtered = butter_lowpass_filter(y_3_no, cutoff, fs, order)\n","\n","    # Calculate RMSE\n","    rmse_1 = np.sqrt(mean_squared_error(y_test_1, y_1_no))\n","    rmse_2 = np.sqrt(mean_squared_error(y_test_2, y_2_no))\n","    rmse_3 = np.sqrt(mean_squared_error(y_test_3, y_3_no))\n","\n","    rmse_1_filtered = np.sqrt(mean_squared_error(y_test_1, y_1_filtered))\n","    rmse_2_filtered = np.sqrt(mean_squared_error(y_test_2, y_2_filtered))\n","    rmse_3_filtered = np.sqrt(mean_squared_error(y_test_3, y_3_filtered))\n","\n","    print(\"RMSE without filter:\", rmse_1, rmse_2, rmse_3)\n","    print(\"RMSE with filter:\", rmse_1_filtered, rmse_2_filtered, rmse_3_filtered)\n","\n","    # Correlation\n","    p_1 = np.corrcoef(y_1_no, y_test_1)[0, 1]\n","    p_2 = np.corrcoef(y_2_no, y_test_2)[0, 1]\n","    p_3 = np.corrcoef(y_3_no, y_test_3)[0, 1]\n","    p_1_filtered = np.corrcoef(y_1_filtered, y_test_1)[0, 1]\n","    p_2_filtered = np.corrcoef(y_2_filtered, y_test_2)[0, 1]\n","    p_3_filtered = np.corrcoef(y_3_filtered, y_test_3)[0, 1]\n","\n","    print(\"\\nCorrelation without filter:\", p_1, p_2, p_3)\n","    print(\"Correlation with filter:\", p_1_filtered, p_2_filtered, p_3_filtered)\n","\n","    return {\n","        'rmse': [rmse_1, rmse_2, rmse_3],\n","        'rmse_filtered': [rmse_1_filtered, rmse_2_filtered, rmse_3_filtered],\n","        'pcc': [p_1, p_2, p_3],\n","        'pcc_filtered': [p_1_filtered, p_2_filtered, p_3_filtered],\n","        'y_no': [y_1_no, y_2_no, y_3_no],\n","        'y_filtered': [y_1_filtered, y_2_filtered, y_3_filtered],\n","        'y_test': [y_test_1, y_test_2, y_test_3]\n","    }\n","\n","# Plot function\n","def plot_predictions(results, title, ax, channel):\n","    y_no = results['y_no'][channel]\n","    y_filtered = results['y_filtered'][channel]\n","    y_test = results['y_test'][channel]\n","\n","    ax.plot(y_test[:3000], label='Ground Truth')\n","    ax.plot(y_no[:3000], label='Predicted (No Filter)')\n","    ax.plot(y_filtered[:3000], label='Predicted (Filtered)')\n","    ax.set_title(title)\n","    ax.set_xlabel('Time (centiseconds)')\n","    ax.set_ylabel('Joint Angle (degrees)')\n","    ax.set_ylim(-180, 180)\n","    ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n","\n","    # Adding RMSE values\n","    rmse = results['rmse'][channel]\n","    rmse_filtered = results['rmse_filtered'][channel]\n","\n","    textstr = '\\n'.join((\n","        f'RMSE (No Filter): {rmse:.3f}',\n","        f'RMSE (Filtered): {rmse_filtered:.3f}',\n","    ))\n","\n","    ax.text(0.98, 0.05, textstr, transform=ax.transAxes, fontsize=10,\n","            verticalalignment='bottom', horizontalalignment='right',\n","            bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white'))\n","\n","# Process file\n","def process_file_student(file_path, student_model):\n","    df = pd.read_csv(file_path)\n","    print(\"processing file {}\".format(file_path))\n","\n","    columns_imu_acc = config.channels_imu_acc\n","    columns_imu_gyr = config.channels_imu_gyr\n","    columns_joints = config.channels_joints\n","\n","    input_acc = df[columns_imu_acc].values\n","    input_gyr = df[columns_imu_gyr].values\n","    input_emg = df[config.channels_emg].values\n","    target_data = df[columns_joints].values\n","\n","    acc_tensor = torch.tensor(input_acc, dtype=torch.float32).unsqueeze(0)\n","    gyr_tensor = torch.tensor(input_gyr, dtype=torch.float32).unsqueeze(0)\n","    emg_tensor = torch.tensor(input_emg, dtype=torch.float32).unsqueeze(0)\n","    target_tensor = torch.tensor(target_data, dtype=torch.float32).unsqueeze(0)\n","\n","    window_size = 100\n","    predictions_teacher = []\n","\n","    with torch.no_grad():\n","        for i in range(0, acc_tensor.shape[1] - window_size + 1, window_size):\n","            acc_window = acc_tensor[:, i:i + window_size, :3].to(device)\n","            gyr_window = gyr_tensor[:, i:i + window_size, :3].to(device)\n","            emg_window = emg_tensor[:, i:i + window_size, :3].to(device)\n","\n","            output, _ = student_model(acc_window, gyr_window)\n","            predictions_teacher.append(output.cpu().numpy())\n","\n","    predictions_teacher = np.concatenate(predictions_teacher, axis=1)\n","    predictions_teacher = predictions_teacher.reshape(-1, predictions_teacher.shape[-1])\n","    target_tensor_np = target_tensor.cpu().numpy().squeeze(0)[:predictions_teacher.shape[0]]\n","\n","    print(f\"predictions_teacher shape: {predictions_teacher.shape}\")\n","    print(f\"target_tensor_np shape: {target_tensor_np.shape}\")\n","\n","    return RMSE_prediction(predictions_teacher, target_tensor_np, target_tensor_np.shape[1])\n","\n","\n","# Main processing and plotting\n","combined_folder_path = f'/content/MyDrive/MyDrive/sd_datacollection_v2/subject_1/combined/'\n","file_paths = glob.glob(os.path.join(combined_folder_path, '*.csv'))\n","print(f'Found {len(file_paths)} files for processing.')\n","\n","num_files = len(file_paths)\n","num_channels = 3\n","\n","fig, axes = plt.subplots(num_files, num_channels, figsize=(15, 5 * num_files))\n","\n","for i, file_path in enumerate(file_paths):\n","    print(f'Processing file: {file_path}')\n","    results = process_file_student(file_path, kd_models[0])\n","\n","    plot_predictions(results, f'Elbow Flexion - {os.path.basename(file_path[:-4])}', axes[i, 0], 0)\n","    plot_predictions(results, f'Arm Flexion - {os.path.basename(file_path[:-4])}', axes[i, 1], 1)\n","    plot_predictions(results, f'Arm Adduction - {os.path.basename(file_path[:-4])}', axes[i, 2], 2)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"S-qR2TUQWJZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_models_and_save_csv(file_path, teacher_model, student_model):\n","    # Load CSV\n","    df = pd.read_csv(file_path)\n","\n","    # Create copies for teacher and student predictions\n","    df_teacher = df.copy()\n","    df_student = df.copy()\n","\n","    # Process file for teacher model\n","    results_teacher = process_file(file_path, teacher_model)\n","    elbow_flex_pred_teacher_filtered = results_teacher['y_filtered'][0]  # Elbow flexion predictions (filtered) from teacher model\n","\n","    # Process file for student model\n","    results_student = process_file_student(file_path, student_model)\n","    elbow_flex_pred_student_filtered = results_student['y_filtered'][0]  # Elbow flexion predictions (filtered) from student model\n","\n","    # Determine the minimum length\n","    min_length = min(len(df), len(elbow_flex_pred_teacher_filtered), len(elbow_flex_pred_student_filtered))\n","\n","    # Tailor predictions to the shorter length\n","    elbow_flex_pred_teacher_filtered = elbow_flex_pred_teacher_filtered[:min_length]\n","    elbow_flex_pred_student_filtered = elbow_flex_pred_student_filtered[:min_length]\n","\n","    # Truncate the dataframes to the minimum length\n","    df_teacher = df_teacher.iloc[:min_length]\n","    df_student = df_student.iloc[:min_length]\n","\n","    # Add predictions to respective dataframes\n","    df_teacher['elbow_flex_pred'] = elbow_flex_pred_teacher_filtered\n","    df_student['elbow_flex_pred'] = elbow_flex_pred_student_filtered\n","\n","    # Save new CSV files\n","    teacher_csv_path = os.path.join('teacher.csv')\n","    student_csv_path = os.path.join( 'student.csv')\n","\n","    df_teacher.to_csv(teacher_csv_path, index=False)\n","    df_student.to_csv(student_csv_path, index=False)\n","\n","    print(f'Teacher predictions saved to {teacher_csv_path}')\n","    print(f'Student predictions saved to {student_csv_path}')"],"metadata":{"id":"UnrvmLaJXO6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teacher_checkpoint = \"/content/MyDrive/MyDrive/models/Vanilla_SD_csv_RMSE_gridsearchtraining_smalldataset_fulldata_[1,2,3,7,9,10]/retrained_teacher_model_train_subject_10_subject_2_subject_3_subject_7_subject_9_test_subject_1_epoch_175.pth\"\n","model_teacher = teacher(18, 18,3)\n","model_teacher.load_state_dict(torch.load(teacher_checkpoint)['model_state_dict'])\n","\n","model_teacher.eval()\n","model_teacher.to(device)\n","\n","file_path = '/content/MyDrive/MyDrive/sd_datacollection_v2/subject_1/combined/P001_T001_elbowflexion_slow_combined.csv'\n","run_models_and_save_csv(file_path, teacher_model=model_teacher, student_model=kd_models[0])"],"metadata":{"id":"lQ4mwYDnbvjo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def RMSE_prediction(yhat_4,test_y, output_dim):\n","\n","  s1=yhat_4.shape[0]*yhat_4.shape[1]\n","\n","  test_o=test_y.reshape((s1,output_dim))\n","  yhat=yhat_4.reshape((s1,output_dim))\n","\n","\n","\n","\n","  y_1_no=yhat[:,0]\n","  y_2_no=yhat[:,1]\n","  y_3_no=yhat[:,2]\n","  #y_4_no=yhat[:,3]\n","  #y_5_no=yhat[:,4]\n","  #y_6_no=yhat[:,5]\n","  #y_7_no=yhat[:,6]\n","  #y_8_no=yhat[:,7]\n","  #y_9_no=yhat[:,8]\n","  #y_10_no=yhat[:,9]\n","\n","\n","  y_1=y_1_no\n","  y_2=y_2_no\n","  y_3=y_3_no\n","  #y_4=y_4_no\n","  #y_5=y_5_no\n","  #y_6=y_6_no\n","  #y_7=y_7_no\n","\n","\n","\n","  y_test_1=test_o[:,0]\n","  y_test_2=test_o[:,1]\n","  y_test_3=test_o[:,2]\n","  #y_test_4=test_o[:,3]\n","  #y_test_5=test_o[:,4]\n","  #y_test_6=test_o[:,5]\n","  #y_test_7=test_o[:,6]\n","  #y_test_8=test_o[:,7]\n","  #y_test_9=test_o[:,8]\n","  #y_test_10=test_o[:,9]\n","\n","\n","\n","\n","\n","  #print(y_1.shape,y_test_1.shape)\n","\n","\n","  cutoff=6\n","  fs=200\n","  order=4\n","\n","  nyq = 0.5 * fs\n","  ## filtering data ##\n","  def butter_lowpass_filter(data, cutoff, fs, order):\n","      normal_cutoff = cutoff / nyq\n","      # Get the filter coefficients\n","      b, a = butter(order, normal_cutoff, btype='low', analog=False)\n","      y = filtfilt(b, a, data)\n","      return y\n","\n","\n","\n","  # y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)\n","  # y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)\n","  # y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)\n","  # y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)\n","  # y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)\n","  # y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)\n","  # y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)\n","  #y_8=butter_lowpass_filter(y_8_no, cutoff, fs, order)\n","  #y_9=butter_lowpass_filter(y_9_no, cutoff, fs, order)\n","  #y_10=butter_lowpass_filter(y_10_no, cutoff, fs, order)\n","\n","\n","\n","\n","  Z_1=y_1\n","  Z_2=y_2\n","  Z_3=y_3\n","  #Z_4=y_4\n","  #Z_5=y_5\n","  #Z_6=y_6\n","  #Z_7=y_7\n","  #Z_8=y_8\n","  #Z_9=y_9\n","  #Z_10=y_10\n","\n","\n","\n","  ###calculate RMSE\n","\n","  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1))))\n","  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2))))\n","  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3))))\n","  #rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100\n","  #rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100\n","  #rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100\n","  #rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100\n","  #rmse_8 =((np.sqrt(mean_squared_error(y_test_8,y_8)))/(max(y_test_8)-min(y_test_8)))*100\n","  #rmse_9 =((np.sqrt(mean_squared_error(y_test_9,y_9)))/(max(y_test_9)-min(y_test_9)))*100\n","  #rmse_10 =((np.sqrt(mean_squared_error(y_test_10,y_10)))/(max(y_test_10)-min(y_test_10)))*100\n","\n","\n","  print(rmse_1)\n","  print(rmse_2)\n","  print(rmse_3)\n","  #print(rmse_4)\n","  #print(rmse_5)\n","  #print(rmse_6)\n","  #print(rmse_7)\n","  #print(rmse_8)\n","  #print(rmse_9)\n","  #print(rmse_10)\n","\n","\n","  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n","  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n","  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n","  #p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n","  #p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n","  #p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n","  #p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n","  #p_8=np.corrcoef(y_8, y_test_8)[0, 1]\n","  #p_9=np.corrcoef(y_9, y_test_9)[0, 1]\n","  #p_10=np.corrcoef(y_10, y_test_10)[0, 1]\n","\n","\n","  print(\"\\n\")\n","  print(p_1)\n","  print(p_2)\n","  print(p_3)\n","  #print(p_4)\n","  #print(p_5)\n","  #print(p_6)\n","  #print(p_7)\n","  #print(p_8)\n","  #print(p_9)\n","  #print(p_10)\n","\n","\n","              ### Correlation ###\n","  p=np.array([p_1,p_2,p_3])\n","  #,p_4,p_5,p_6,p_7])\n","\n","\n","\n","\n","      #### Mean and standard deviation ####\n","\n","  rmse=np.array([rmse_1,rmse_2,rmse_3])\n","  #,rmse_4,rmse_5,rmse_6,rmse_7])\n","\n","      #### Mean and standard deviation ####\n","  m=statistics.mean(rmse)\n","  SD=statistics.stdev(rmse)\n","  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n","\n","  m_c=statistics.mean(p)\n","  SD_c=statistics.stdev(p)\n","  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n","\n","\n","\n","  return rmse, p, Z_1,Z_2,Z_3\n","  #,Z_4,Z_5,Z_6,Z_7\n","\n","\n","\n","############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n"],"metadata":{"id":"ycoxGqFdrwzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teacher_checkpoint = \"/content/MyDrive/MyDrive/models/Vanilla_SD_csv_RMSE_gridsearchtraining_smalldataset_fulldata_[1,2,3,7,9,10]/retrained_teacher_model_train_subject_10_subject_2_subject_3_subject_7_subject_9_test_subject_1_epoch_175.pth\"\n","model_teacher = teacher(18, 18,3)\n","model_teacher.load_state_dict(torch.load(teacher_checkpoint)['model_state_dict'])\n","\n","model_teacher.eval()\n","model_teacher.to(device)\n","\n","# Function to evaluate model and save RMSE and PCC per channel\n","def evaluate_student_and_save_rmse_per_channel(model, loader):\n","    model.eval()\n","    yhat_5 = []\n","    test_target = []\n","\n","    with torch.no_grad():\n","        for data_acc, data_gyr, target, data_EMG in loader:\n","            output, _ = model(data_acc[:,:,:3].to(device).float(), data_gyr[:,:,:3].to(device).float())\n","            yhat_5.append(output)\n","            test_target.append(target)\n","\n","            # Clear memory\n","            del data_acc, data_gyr, data_EMG, target, output\n","            torch.cuda.empty_cache()\n","\n","    yhat_4 = torch.cat(yhat_5, dim=0).detach().cpu().numpy()\n","    test_target = torch.cat(test_target, dim=0).detach().cpu().numpy()\n","\n","    num_channels = yhat_4.shape[2]\n","    rmse_per_channel = np.zeros(num_channels)\n","    pcc_per_channel = np.zeros(num_channels)\n","\n","    for channel in range(num_channels):\n","        rmse_per_channel[channel] = np.sqrt(mean_squared_error(test_target[:, :, channel], yhat_4[:, :, channel]))\n","        pcc_per_channel[channel] = np.corrcoef(test_target[:, :, channel].flatten(), yhat_4[:, :, channel].flatten())[0, 1]\n","\n","    print(f'RMSE per channel: {rmse_per_channel}')\n","    print(f'PCC per channel: {pcc_per_channel}')\n","\n","    rmse, p, Z_1, Z_2, Z_3 = RMSE_prediction(yhat_4, test_target, output_dim=3)\n","    return rmse_per_channel, rmse, p, pcc_per_channel\n","\n","# Function to save model checkpoints\n","def save_checkpoint(model, optimizer, epoch, train_losses, val_losses, filepath):\n","    state = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'train_losses': train_losses,\n","        'val_losses': val_losses\n","    }\n","    torch.save(state, filepath)\n","    print(f\"Checkpoint saved at epoch {epoch} to {filepath}\")\n","\n","# Initialize variables and models\n","train_losses = []\n","val_losses = []\n","train_channel_losses = []\n","val_channel_losses = []\n","train_pccs = []\n","val_pccs = []\n","\n","student_model_KD = create_student_model_KD(input_size=3)\n","student_model_KD.to(device)\n","\n","optimizer = torch.optim.Adam(student_model_KD.parameters(), lr=config.lr)\n","criterion = nn.MSELoss()\n","\n","results_dir = \"/content/MyDrive/MyDrive/models/student1[1,2,3,7,9,10]\"\n","\n","if not os.path.exists(results_dir):\n","    os.makedirs(results_dir)\n","\n","for epoch in range(0, 201):\n","    student_model_KD.train()\n","    epoch_train_loss = 0.0  # Initialize as a float\n","    epoch_train_channel_loss = np.zeros(3)  # Assuming 3 channels\n","    epoch_train_pcc = np.zeros(3)  # Assuming 3 channels\n","\n","    for data_acc, data_gyr, target, data_EMG in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\"):\n","        optimizer.zero_grad()\n","        student_output, x_student_1 = student_model_KD(data_acc[:, :, :3].to(device).float(), data_gyr[:, :, :3].to(device).float())\n","\n","        with torch.no_grad():\n","            data_acc_device = data_acc.to(device).float()\n","            data_gyr_device = data_gyr.to(device).float()\n","            data_EMG_device = data_EMG.to(device).float()\n","            teacher_output, x_teacher_1 = model_teacher(data_acc_device, data_gyr_device, data_EMG_device)\n","\n","        target_device = target.to(device).float()\n","        loss = criterion(student_output, target_device) + alpha * criterion(student_output, teacher_output) + beta * criterion(x_student_1, x_teacher_1)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_train_loss += loss.item()\n","\n","        batch_loss = np.mean((student_output.detach().cpu().numpy() - target.detach().cpu().numpy()) ** 2, axis=(0, 1))\n","        epoch_train_channel_loss += batch_loss\n","\n","        batch_pcc = compute_pcc(student_output.detach().cpu().numpy(), target.detach().cpu().numpy())\n","        epoch_train_pcc += batch_pcc\n","\n","    epoch_train_loss /= len(train_loader)\n","    train_losses.append(epoch_train_loss)\n","    train_channel_losses.append(epoch_train_channel_loss / len(train_loader))\n","    train_pccs.append(epoch_train_pcc / len(train_loader))\n","\n","    # Evaluate and save checkpoint every 1 epoch\n","    if epoch % 25 == 0:\n","        student_model_KD.eval()\n","        epoch_val_loss = 0\n","        epoch_val_channel_loss = np.zeros(3)  # Assuming 3 channels\n","        epoch_val_pcc = np.zeros(3)  # Assuming 3 channels\n","\n","        with torch.no_grad():\n","            for data_acc, data_gyr, target, data_EMG in val_loader:\n","                output, _ = student_model_KD(data_acc[:, :, :3].to(device).float(), data_gyr[:, :, :3].to(device).float())\n","                loss = criterion(output, target.to(device).float())\n","                epoch_val_loss += loss.item()\n","\n","                batch_loss = np.mean((output.detach().cpu().numpy() - target.detach().cpu().numpy()) ** 2, axis=(0, 1))\n","                epoch_val_channel_loss += batch_loss\n","\n","                batch_pcc = compute_pcc(output.detach().cpu().numpy(), target.detach().cpu().numpy())\n","                epoch_val_pcc += batch_pcc\n","\n","        val_losses.append(epoch_val_loss / len(val_loader))\n","        val_channel_losses.append(epoch_val_channel_loss / len(val_loader))\n","        val_pccs.append(epoch_val_pcc / len(val_loader))\n","\n","        checkpoint_path = os.path.join(results_dir, f'student_model_KD_train_{config.train_subjects}_test_{config.test_subjects}_epoch_{epoch}.pth')\n","        save_checkpoint(student_model_KD, optimizer, epoch, train_losses, val_losses, checkpoint_path)\n","\n","        # Evaluate on test set\n","        print(f\"________________________________\\nEpoch {epoch} Test set metrics\")\n","        test_results = evaluate_student_and_save_rmse_per_channel(student_model_KD, test_loader)\n","        test_results_df = pd.DataFrame({\n","            'rmse_per_channel': test_results[0],\n","            'test_rmse': test_results[1],\n","            'test_p': test_results[2],\n","            'pcc_per_channel': test_results[3]\n","        })\n","        test_results_df.to_csv(os.path.join(results_dir, f'test_results_{config.train_subjects}_test_{config.test_subjects}_epoch_{epoch}.csv'), index=False)\n","\n","\n"],"metadata":{"id":"dl5LRGLKe8Ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unsqueezing lists to pass to plot_results\n","train_channel_losses = [train_channel_losses]\n","val_channel_losses = [val_channel_losses]\n","train_pccs = [train_pccs]\n","val_pccs = [val_pccs]\n","\n","# Ensure that all lists have the same length\n","min_length = min(len(train_channel_losses[0]), len(val_channel_losses[0]), len(train_pccs[0]), len(val_pccs[0]))\n","\n","train_channel_losses = [train_channel_losses[0][:min_length]]\n","val_channel_losses = [val_channel_losses[0][:min_length]]\n","train_pccs = [train_pccs[0][:min_length]]\n","val_pccs = [val_pccs[0][:min_length]]\n","\n","plot_results(train_channel_losses, val_channel_losses, train_pccs, val_pccs, config)"],"metadata":{"id":"LlFTVR4esT5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Disconnect and delete the runtime\n","import os\n","from google.colab import runtime\n","\n","runtime.unassign()\n"],"metadata":{"id":"JynlI9VVwqf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CEHOx82fyCRv"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1rdHb4TuCnnIDmoPaIx_xt1QlqeoqZ3aJ","timestamp":1725865634644},{"file_id":"1zVQFZK4F4nFC3rAsfc5f276kkLKyabA7","timestamp":1725770443175},{"file_id":"1_srYfBgGy8FQIMSohL9HL3Ac4ZZfvPVF","timestamp":1722559818032},{"file_id":"1ueeVtfayoqaNooAbjpcCBoKOWg7UmxAp","timestamp":1722359381849},{"file_id":"1ryl9H3tW6u9DyNInb-iS82rOZ4anjgZt","timestamp":1722295666388},{"file_id":"1lyKGsrpoLMhWE9Qz6A7qFZ5-o3fVuSVw","timestamp":1722291006477},{"file_id":"1JhajboXIAvcWgKNCN4Ljlg-Ebih6rbi3","timestamp":1722268029267},{"file_id":"1-tWEKDHgFp0R-NvBdAJIIFHZKc6185I4","timestamp":1722201240061},{"file_id":"1r91HidleatpLF4x2rdc9DnxWyb-U9exV","timestamp":1722197547794},{"file_id":"1sWKusmF7ocIZanW6vcld-Mr6bzERe-kb","timestamp":1722196228475},{"file_id":"1nzXq_89_RbuU-OR3LFr-idc_Gl8aypPt","timestamp":1722195060257},{"file_id":"1v8w64kwmH2zehSmEaUGsLZqJNEwecL-i","timestamp":1722185530003},{"file_id":"1QuAo2poyCHl3DFfE8Wrj9OnfDzv5HgpR","timestamp":1722181752384},{"file_id":"11qGVlcaE5KShLP8boMFZU_pIAYHJv2N1","timestamp":1722113928372},{"file_id":"1fzC0avZyr80RIwPOYEeAtJIlD_xB0-zv","timestamp":1722100536152}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}